{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TROPOMI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_download(input_type, bbox, date, product_type, component_nom):\n",
    "\n",
    "   \"\"\" Query and download the TROPOMI dataset from Sentinel API\n",
    "\n",
    "         Args:\n",
    "            input_type (str): Search type (Manual or Query)\n",
    "            bbox (arr): Query bounding box\n",
    "            date (list or tuple): Query date\n",
    "            product_type (str): Query product type\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "\n",
    "        Returns:\n",
    "            product_name (str): Product name of TROPOMI product within 5phub\n",
    "   \"\"\"\n",
    "\n",
    "   user = 's5pguest' \n",
    "   password = 's5pguest' \n",
    "   api = SentinelAPI(user, password, 'https://s5phub.copernicus.eu/dhus/')\n",
    "\n",
    "   if input_type == 'Manual':\n",
    "\n",
    "      file_name = input('Write file name: ')\n",
    "      product_name = input('Write product name:')\n",
    "\n",
    "   elif input_type == 'Query':\n",
    "      \n",
    "      print('WARNING: The maximum number of items that can be shown is 5.')\n",
    "      print('You can see all the results at https://s5phub.copernicus.eu/dhus/.')\n",
    "\n",
    "      poly = geojson.Polygon([[(bbox[0][0], bbox[0][1]), (bbox[0][0], bbox[1][1]), \n",
    "                               (bbox[1][0], bbox[1][1]), (bbox[1][0], bbox[0][1]), \n",
    "                               (bbox[0][0], bbox[0][1])]])\n",
    "\n",
    "      products = api.query(area = geojson_to_wkt(poly),\n",
    "                           area_relation = 'Contains',\n",
    "                           producttype = product_type,\n",
    "                           processinglevel = 'L2',\n",
    "                           platformname = 'Sentinel-5 Precursor',\n",
    "                           instrumentname = 'TROPOspheric Monitoring Instrument',\n",
    "                           processingmode = 'Near real time',\n",
    "                           date = date,\n",
    "                           limit = 5)\n",
    "\n",
    "      items = list(products.items())\n",
    "      \n",
    "      if items:\n",
    "         for i in range(0, len(items)):\n",
    "               print('Number ', i, '-', items[i][1]['title'], sep = '')\n",
    "\n",
    "      else: \n",
    "         print('There are no results for the processing mode NRT. The search in the offline archives will start.')\n",
    "         products = api.query(area = geojson_to_wkt(poly),\n",
    "                              area_relation = 'Contains',\n",
    "                              producttype = product_type,\n",
    "                              processinglevel = 'L2',\n",
    "                              platformname = 'Sentinel-5 Precursor',\n",
    "                              instrumentname = 'TROPOspheric Monitoring Instrument',\n",
    "                              processingmode = 'Offline',\n",
    "                              date = date,\n",
    "                              limit = 5)\n",
    "\n",
    "         items = list(products.items())\n",
    "\n",
    "         if items:\n",
    "            for i in range(0, len(items)):\n",
    "               print('Number ', i, '-', items[i][1]['title'], sep = '')\n",
    "\n",
    "         else: \n",
    "            print('There are no results in the offline archives. The code will be interrupted.')\n",
    "            raise KeyboardInterrupt\n",
    "            \n",
    "      file_int = input('Select number or press Enter if you want to select the first result: ') or 0\n",
    "      file_name = items[int(file_int)][0]\n",
    "      product_name = items[int(file_int)][1]['title'] + '.nc'\n",
    "\n",
    "   print('SELECTED')\n",
    "   print('File name:', file_name)\n",
    "   print('Product name:', product_name)\n",
    "   \n",
    "   if os.path.isfile(os.path.join('/', '/'.join(os.getcwd().split('/')[1:3]), 'adc-toolbox', os.path.relpath('data/tropomi' + '/' + \n",
    "                     component_nom + '/' + date[0].split('T')[0] + '/' + product_name))):\n",
    "      print('The file exists, it will not be downloaded again.')\n",
    "\n",
    "   else:\n",
    "      print('The file does not exist, it will be downloaded.')\n",
    "      print(f'Downloading {product_name}...')\n",
    "      api.download(file_name, directory_path = os.path.join('/', '/'.join(\n",
    "                                               os.getcwd().split('/')[1:3]), 'adc-toolbox', \n",
    "                                               os.path.relpath('data/tropomi/' + component_nom + '/' + date[0].split('T')[0])))\n",
    "\n",
    "   return product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_read(dates, component_nom, sensor_column):\n",
    "\n",
    "    \"\"\" Read TROPOMI dataset as xarray dataset object\n",
    "\n",
    "        Args:\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            sensor_column (str): Name of sensor column in downloaded dataset\n",
    "\n",
    "        Returns:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "            support_input_ds (xarray): TROPOMI dataset that contains support input data in xarray format\n",
    "            support_details_ds (xarray): TROPOMI dataset that contains support details data in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    sensor_ds_all = []\n",
    "    support_input_ds_all = []\n",
    "    support_details_ds_all = []\n",
    "\n",
    "    for date in dates:\n",
    "\n",
    "        path = os.path.join('/', '/'.join(\n",
    "               os.getcwd().split('/')[1:3]), 'adc-toolbox', \n",
    "               os.path.relpath('data/tropomi/' + component_nom + '/' + date[0].split('T')[0]))\n",
    "        product_names = [file for file in os.listdir(path) if file.endswith('.nc')]\n",
    "        \n",
    "        for product_name in product_names:\n",
    "\n",
    "            sensor_ds = xr.open_dataset(path + '/' + product_name, group = 'PRODUCT')\n",
    "            sensor_ds = sensor_ds.rename({sensor_column: 'sensor_column'})\n",
    "            \n",
    "            support_input_ds = xr.open_dataset(path + '/' + product_name, \n",
    "                                               group = 'PRODUCT/SUPPORT_DATA/INPUT_DATA')\n",
    "\n",
    "            support_input_ds = support_input_ds.assign(ground_pixel = sensor_ds.ground_pixel)\n",
    "            support_input_ds = support_input_ds.assign(scanline = sensor_ds.scanline)\n",
    "            support_input_ds = support_input_ds.assign(time = sensor_ds.time)\n",
    "            support_input_ds = support_input_ds.set_coords(['ground_pixel', 'scanline', 'time'])\n",
    "\n",
    "            support_details_ds = xr.open_dataset(path + '/' + product_name, \n",
    "                                                 group = 'PRODUCT/SUPPORT_DATA/DETAILED_RESULTS')\n",
    "\n",
    "            support_details_ds = support_details_ds.assign(ground_pixel = sensor_ds.ground_pixel)\n",
    "            support_details_ds = support_details_ds.assign(scanline = sensor_ds.scanline)\n",
    "            support_details_ds = support_details_ds.assign(time = sensor_ds.time)\n",
    "            support_details_ds = support_details_ds.set_coords(['ground_pixel', 'scanline', 'time'])\n",
    "\n",
    "            if component_nom == 'CO':\n",
    "                    \n",
    "                # Transform heights into levels\n",
    "                data = {'Layer': np.arange(1, 51)[::-1], 'Height': sensor_ds.layer}\n",
    "                dataframe = pd.DataFrame(data)\n",
    "                \n",
    "                for i in range(0, 50):\n",
    "\n",
    "                    sensor_ds['layer'] = xr.where(sensor_ds.layer == dataframe['Height'].iloc[i], \n",
    "                                                  int(dataframe['Layer'].iloc[i]), sensor_ds['layer'])\n",
    "\n",
    "            sensor_ds_all.append(sensor_ds)\n",
    "            support_input_ds_all.append(support_input_ds)\n",
    "            support_details_ds_all.append(support_details_ds)\n",
    "\n",
    "    sensor_ds = xr.concat(sensor_ds_all, dim = 'time')\n",
    "    support_input_ds = xr.concat(support_input_ds_all, dim = 'time')\n",
    "    support_details_ds = xr.concat(support_details_ds_all, dim = 'time')\n",
    "\n",
    "    return sensor_ds, support_input_ds, support_details_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_pressure(sensor_ds, component_nom, support_input_ds, support_details_ds):\n",
    "\n",
    "    \"\"\" Calculate level pressures for TROPOMI dataset\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            support_input_ds (xarray): TROPOMI dataset that contains support input data in xarray format\n",
    "            support_details_ds (xarray): TROPOMI dataset that contains support details data in xarray format\n",
    "            \n",
    "        Returns:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    if component_nom == 'NO2':\n",
    "        \n",
    "        print('The level pressures will be calculated.')\n",
    "\n",
    "        # Calculate pressure as p = ap + b * ps (Units: ap(Pa) + b(none) * ps(Pa) -> To Pa)\n",
    "        pressure_upper_bound = (sensor_ds.tm5_constant_a.sel(vertices = 1) + \n",
    "                                sensor_ds.tm5_constant_b.sel(vertices = 1) * support_input_ds.surface_pressure)\n",
    "\n",
    "        pressure_lower_bound = (sensor_ds.tm5_constant_a.sel(vertices = 0) + \n",
    "                                sensor_ds.tm5_constant_b.sel(vertices = 0) * support_input_ds.surface_pressure)\n",
    "\n",
    "        sensor_ds = sensor_ds.assign(pressure = (pressure_upper_bound + pressure_lower_bound) / 2)\n",
    "    \n",
    "    elif component_nom == 'CO':\n",
    "\n",
    "        print('The level pressures will be retrieved.')\n",
    "\n",
    "        # Pressure is at lower bound!\n",
    "        pressure_lower = support_details_ds.pressure_levels\n",
    "        sensor_ds = sensor_ds.assign(pressure = pressure_lower)\n",
    "\n",
    "    elif component_nom == 'SO2':\n",
    "        \n",
    "        print('The level pressures will be calculated.')\n",
    "        \n",
    "        # Unknown bound, half?\n",
    "        pressure = (support_input_ds.tm5_constant_a + \n",
    "                    support_input_ds.tm5_constant_b * support_input_ds.surface_pressure)\n",
    "        sensor_ds = sensor_ds.assign(pressure = pressure)\n",
    "    \n",
    "    elif component_nom == 'O3':\n",
    "\n",
    "        print('The level pressures will be retrieved.')\n",
    "\n",
    "        # Unknown bound, half?\n",
    "        pressure = support_details_ds.pressure_grid\n",
    "        sensor_ds = sensor_ds.assign(pressure = pressure)\n",
    "\n",
    "    else:\n",
    "        print('This dataset does not contain data to retrieve or calculate the level pressures.')\n",
    "\n",
    "    return sensor_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_column_kernel(sensor_ds, component_nom, support_details_ds):\n",
    "\n",
    "    \"\"\" Calculate column kernels for TROPOMI dataset\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            support_details_ds (xarray): TROPOMI dataset that contains support details data in xarray format\n",
    "\n",
    "        Returns:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    if component_nom == 'NO2':\n",
    "\n",
    "        print('The column kernels will be calculated.')\n",
    "\n",
    "        column_kernel = xr.where(sensor_ds.layer > sensor_ds.tm5_tropopause_layer_index, 0, \n",
    "                                 sensor_ds.averaging_kernel * (sensor_ds.air_mass_factor_total / \n",
    "                                 sensor_ds.air_mass_factor_troposphere))\n",
    "        sensor_ds = sensor_ds.assign(column_kernel = column_kernel)\n",
    "\n",
    "    elif component_nom == 'CO':\n",
    "        \n",
    "        print('The column kernels will be calculated.')\n",
    "\n",
    "        column_kernel = support_details_ds.column_averaging_kernel\n",
    "        sensor_ds = sensor_ds.assign(column_kernel = column_kernel)\n",
    "    \n",
    "    elif component_nom == 'SO2':\n",
    "        \n",
    "        print('The column kernels will be calculated.')\n",
    "\n",
    "        \"\"\"\n",
    "        height_options = [1, 7, 15]\n",
    "        height = input('Input height (in km) to calculate the column kernels with accuracy: ')\n",
    "\n",
    "        while int(height) not in height_options:\n",
    "            print('ERROR: Enter a valid height number. The options are 1, 7 or 15 km.')\n",
    "            height = input('Input height (in km): ')\n",
    "        \"\"\"\n",
    "\n",
    "        column_kernel = support_details_ds.averaging_kernel\n",
    "        sensor_ds = sensor_ds.assign(column_kernel = column_kernel)\n",
    "    \n",
    "    elif component_nom == 'O3':\n",
    "\n",
    "        print('The column kernels will be calculated.')\n",
    "        \n",
    "        column_kernel = support_details_ds.averaging_kernel\n",
    "        sensor_ds = sensor_ds.assign(column_kernel = column_kernel)\n",
    "\n",
    "    else:\n",
    "        print('The dataset does not contain data to retrieve or calculate the column averaging kernels.')\n",
    "\n",
    "    return sensor_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_apriori_profile(sensor_ds, component, support_details_ds):\n",
    "\n",
    "    \"\"\" Retrieve apriori profile if it exists and add to xarray dataset.\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "            component (str): Component name\n",
    "            support_details_ds (xarray): TROPOMI dataset that contains support details data in xarray format\n",
    "        \n",
    "        Returns:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    apriori_profile = component.replace('_', '') + '_profile_apriori'\n",
    "\n",
    "    if apriori_profile in list(support_details_ds.keys()):\n",
    "        apriori_profile = support_details_ds[apriori_profile]\n",
    "        sensor_ds = sensor_ds.assign(apriori_profile = apriori_profile)\n",
    "\n",
    "    else:\n",
    "        print('The dataset does not contain any apriori profile.')\n",
    "    \n",
    "    return sensor_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_lookup_table(sensor_ds, component_nom, time):\n",
    "\n",
    "    \"\"\" Create file with the original corresponding coordinates to each scanline and ground pixel in TROPOMI dataset\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): TROPOMI dataset in xarray format\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            time (timestamp): Start datetime for each period\n",
    "    \"\"\"\n",
    "\n",
    "    # Dims and vars to drop\n",
    "    if component_nom == 'NO2':\n",
    "        dims_to_drop = ['polynomial_exponents', 'vertices', 'intensity_offset_polynomial_exponents', 'corner', 'layer']\n",
    "        vars_to_drop = ['time_utc', 'qa_value', 'sensor_column', 'nitrogendioxide_tropospheric_column_precision', \n",
    "                        'nitrogendioxide_tropospheric_column_precision_kernel', 'air_mass_factor_troposphere', \n",
    "                        'air_mass_factor_total', 'tm5_tropopause_layer_index']\n",
    "\n",
    "    elif component_nom == 'O3':\n",
    "        dims_to_drop = ['corner', 'layer', 'level']\n",
    "        vars_to_drop = ['time_utc', 'qa_value', 'sensor_column', 'ozone_total_vertical_column_precision']\n",
    "\n",
    "    # Create lookup table\n",
    "    lookup_table = sensor_ds.drop_dims(dims_to_drop).drop_vars(vars_to_drop).to_dataframe().reset_index()\n",
    "\n",
    "    # Save as csv\n",
    "    time_str = str(time).split('T')[0]\n",
    "    lookup_table.to_csv(os.path.join('/', '/'.join(\n",
    "                        os.getcwd().split('/')[1:3]), 'adc-toolbox', \n",
    "                        os.path.relpath('data/tropomi/' + component_nom + '/' + time_str + '/' + \n",
    "                        component_nom + '-' + time_str + '-lookup-table.csv')), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_subset(sensor_ds_time, bbox, time, component_nom):\n",
    "\n",
    "    \"\"\" Read file with the corresponding coordinates to each scanline and ground pixel in TROPOMI dataset.\n",
    "        Subset dataset into desired bounding box.\n",
    "\n",
    "        Args:\n",
    "            sensor_ds_time (xarray): TROPOMI dataset in xarray format per time\n",
    "            bbox (arr): Query bounding box\n",
    "            time (timestamp): Start datetime for each period\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "    \n",
    "        Returns:\n",
    "            sensor_ds_time (xarray): TROPOMI dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    time_str = str(time).split('T')[0]\n",
    "    if os.path.isfile(os.path.join('/', '/'.join(\n",
    "                      os.getcwd().split('/')[1:3]), 'adc-toolbox', \n",
    "                      os.path.relpath('data//tropomi/' + component_nom + '/' \n",
    "                      + time_str + '/' + component_nom + '-' + time_str + '-lookup-table.csv'))):\n",
    "        pass\n",
    "\n",
    "    else: \n",
    "        TROPOMI_lookup_table(sensor_ds_time, component_nom, time)\n",
    "        \n",
    "    # Read csv\n",
    "    sensor_coords_df = pd.read_csv(os.path.join('/', '/'.join(\n",
    "                                   os.getcwd().split('/')[1:3]), 'adc-toolbox', \n",
    "                                   os.path.relpath('data/tropomi/' + component_nom + '/' + time_str + '/' + \n",
    "                                   component_nom + '-' + time_str + '-lookup-table.csv')))\n",
    "    \n",
    "    # Set limits\n",
    "    sensor_coords_df = sensor_coords_df[(sensor_coords_df['latitude'] >= bbox[0][1]) & \n",
    "                                        (sensor_coords_df['latitude'] <= bbox[1][1])]\n",
    "    sensor_coords_df = sensor_coords_df[(sensor_coords_df['longitude'] >= bbox[0][0]) & \n",
    "                                        (sensor_coords_df['longitude'] <= bbox[1][0])]\n",
    "    \n",
    "    if sensor_coords_df.empty:\n",
    "        \n",
    "        print('ERROR: The subset could not be made. Try for another TROPOMI dataset. The code will be interrupted.')\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Get scanline and ground pixel coordinates\n",
    "        scanline_coords = np.unique(sensor_coords_df['scanline'].values).tolist()\n",
    "        ground_pixel_coords = np.unique(sensor_coords_df['ground_pixel'].values).tolist()\n",
    "\n",
    "        # Set limits\n",
    "        sensor_ds_time = sensor_ds_time.sel(scanline = scanline_coords, ground_pixel = ground_pixel_coords)\n",
    "\n",
    "    return sensor_ds_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_subset_lookup_table(sensor_ds_time, component_nom):\n",
    "\n",
    "    \"\"\" Create file with the subset corresponding coordinates to each scanline and ground pixel in TROPOMI dataset\n",
    "\n",
    "        Args:\n",
    "            sensor_ds_time (xarray): TROPOMI dataset in xarray format per time\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "        \n",
    "        Returns:\n",
    "            lookup_table (dataframe): Dataframe with subset coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    # Dims and vars to drop\n",
    "    if component_nom == 'NO2':\n",
    "        dims_to_drop = ['polynomial_exponents', 'vertices', 'intensity_offset_polynomial_exponents', 'corner', 'layer']\n",
    "        vars_to_drop = ['time_utc', 'qa_value', 'sensor_column', 'nitrogendioxide_tropospheric_column_precision', \n",
    "                        'nitrogendioxide_tropospheric_column_precision_kernel', 'air_mass_factor_troposphere', \n",
    "                        'air_mass_factor_total', 'tm5_tropopause_layer_index']\n",
    "\n",
    "    elif component_nom == 'O3':\n",
    "        dims_to_drop = ['corner', 'layer', 'level']\n",
    "        vars_to_drop = ['time_utc', 'qa_value', 'sensor_column', 'ozone_total_vertical_column_precision']\n",
    "\n",
    "    # Create lookup table for each time after subset\n",
    "    lookup_table = sensor_ds_time.drop_dims(dims_to_drop).drop_vars(vars_to_drop).to_dataframe().reset_index()\n",
    "\n",
    "    return lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TROPOMI_apply_kernels(match_df, model_ds_time, sensor_ds_time, component_nom):\n",
    "\n",
    "    \"\"\" Apply averaging kernels: Find the nearest neighbours in the observation space \n",
    "        (in latitude and longitudes) and interpolate values in pressure\n",
    "\n",
    "        Args:\n",
    "            match_df (dataframe): Dataframe used to apply averaging kernels\n",
    "            model_ds_time (xarray): Model levels dataset in xarray format per time\n",
    "            sensor_ds_time (xarray): TROPOMI dataset in xarray format per time\n",
    "            \n",
    "        Returns:\n",
    "            match_df (dataframe): Dataframe used to apply averaging kernels\n",
    "    \"\"\"\n",
    "\n",
    "    print('WARNING: The application of averaging kernels will take some time.')\n",
    "    \n",
    "    # Read new coordinates (after subset)\n",
    "    sensor_coords_df = TROPOMI_subset_lookup_table(sensor_ds_time, component_nom)\n",
    "\n",
    "    # Set the layer coordinate as index\n",
    "    match_df = match_df.set_index('layer', append = True)\n",
    "\n",
    "    # Create index that includes CAMS pressure levels for all the locations in TROPOMI\n",
    "    new_array = np.concatenate([np.arange(1, 137) * 1000, sensor_ds_time.layer.values])\n",
    "    new_index = pd.MultiIndex.from_product([match_df.index.levels[0], \n",
    "                                            match_df.index.levels[1],\n",
    "                                            match_df.index.levels[2],\n",
    "                                            new_array\n",
    "                                            ],\n",
    "                                            names = ['scanline', 'ground_pixel', 'time', 'layer'])\n",
    "    \n",
    "    # Append original and new indexes and reindex dataframe\n",
    "    match_df = match_df[~match_df.index.duplicated()]\n",
    "    match_df = match_df.reindex(match_df.index.append(new_index))\n",
    "    \n",
    "    # Sort and reset index\n",
    "    match_df = match_df.sort_index()\n",
    "    match_df = match_df.reset_index()\n",
    "\n",
    "    # Find latitudes in CAMS rows with scanlines and ground pixels\n",
    "    match_df['latitude'] = match_df.apply(lambda row: float(sensor_coords_df[\n",
    "                                                            (sensor_coords_df['scanline'] == row['scanline']) & \n",
    "                                                            (sensor_coords_df['ground_pixel'] == row['ground_pixel'])]['latitude'])\n",
    "                                                            if pd.isnull(row['latitude']) else row['latitude'], \n",
    "                                                            axis = 1)\n",
    "                                                            \n",
    "    # Find longitudes in CAMS rows with scanlines and ground pixels\n",
    "    match_df['longitude'] = match_df.apply(lambda row: float(sensor_coords_df[\n",
    "                                                            (sensor_coords_df['scanline'] == row['scanline']) & \n",
    "                                                            (sensor_coords_df['ground_pixel'] == row['ground_pixel'])]['longitude'])\n",
    "                                                            if pd.isnull(row['longitude']) else row['longitude'], \n",
    "                                                            axis = 1)\n",
    "\n",
    "    # Get unique timestep\n",
    "    sensor_times = sensor_ds_time.delta_time.isel(scanline = 0).values\n",
    "    model_times = model_ds_time.valid_time.values\n",
    "    unique_step = int(np.unique(nearest_neighbour(model_times, sensor_times)))\n",
    "    unique_time = model_ds_time.component.isel(step = unique_step).step.values.astype('timedelta64[h]')\n",
    "\n",
    "    # Get CAMS model partial columns above each level at closest TROPOMI locations (nearest neighbours)\n",
    "    match_df['model_partial_column_above'] = match_df.apply(lambda row: model_ds_time.component.sel(\n",
    "                                                                        step = unique_time,\n",
    "                                                                        hybrid = row['layer'] / 1000, \n",
    "                                                                        latitude = row['latitude'], \n",
    "                                                                        longitude = row['longitude'], \n",
    "                                                                        method = 'nearest').values \n",
    "                                                                        if pd.isnull(row['sensor_column']) else math.nan,\n",
    "                                                                        axis = 1)\n",
    "\n",
    "    # Get CAMS model level pressures\n",
    "    match_df['pressure'] = match_df.apply(lambda row: model_ds_time.pressure.sel(\n",
    "                                                      step = unique_time,\n",
    "                                                      hybrid = row['layer'] / 1000, \n",
    "                                                      latitude = row['latitude'], \n",
    "                                                      longitude = row['longitude'], \n",
    "                                                      method = 'nearest').values \n",
    "                                                      if pd.isnull(row['pressure']) else row['pressure'],\n",
    "                                                      axis = 1)\n",
    "\n",
    "    # Transform 1D-array data to float\n",
    "    match_df['model_partial_column_above'] = match_df['model_partial_column_above'].apply(lambda x: float(x))\n",
    "    match_df['pressure'] = match_df['pressure'].apply(lambda x: float(x))\n",
    "\n",
    "    # Set multiindex again and sort for interpolation\n",
    "    match_df = match_df.reset_index()\n",
    "    match_df = match_df.set_index(['time', 'ground_pixel', 'scanline', 'pressure'])\n",
    "    match_df = match_df.sort_values(['time', 'ground_pixel','scanline', 'pressure'], \n",
    "                                    ascending = [True, True, True, False])\n",
    "\n",
    "    # Interpolate partial columns onto the TM5 pressure levels.\n",
    "    match_df = match_df[~match_df.index.duplicated()]\n",
    "    match_df['model_partial_column_above'] = match_df['model_partial_column_above'].interpolate()\n",
    "\n",
    "    # Drop unnecessary values\n",
    "    match_df = match_df.reset_index()\n",
    "    match_df = match_df.set_index(['time', 'ground_pixel', 'scanline', 'layer'])\n",
    "    match_df = match_df.drop(np.arange(1, 137) * 1000, level = 'layer')     \n",
    "    \n",
    "    # Calculate CAMS partial columns for each TM5 layer (as difference of the interpolated values)\n",
    "    match_df['model_column'] =  match_df['model_partial_column_above'] - match_df['model_partial_column_above'].shift(-1)\n",
    "    match_df = match_df.reset_index()\n",
    "    match_df.loc[match_df['layer'] == 33, ['model_column']] = match_df['model_partial_column_above']\n",
    "    match_df = match_df.set_index(['time', 'ground_pixel', 'scanline', 'layer'])\n",
    "\n",
    "    # Calculate values to generate CAMS column to sum in the next step\n",
    "    if 'apriori_profile' in match_df.columns:\n",
    "        match_df['model_column'] = match_df.apply(lambda row: row['apriori_profile'] +\n",
    "                                                              row['column_kernel'] * row['model_column']  -\n",
    "                                                              row['column_kernel'] * row['apriori_profile'], \n",
    "                                                              axis = 1)\n",
    "    \n",
    "    else:\n",
    "        match_df['model_column'] = match_df.apply(lambda row: row['model_column'] * \n",
    "                                                              row['column_kernel'], \n",
    "                                                              axis = 1)\n",
    "\n",
    "    return match_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84d6139589c0d952b978fb9437b8e11e4e03996aa73ed56a307e4c0ad5e273d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env-new': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
