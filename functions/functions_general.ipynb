{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_check(sensor, model, component_nom, model_full_name, sensor_type):\n",
    "\n",
    "    \"\"\" Check if the comparison is possible\n",
    "\n",
    "        Args:\n",
    "            sensor (str): Name of the sensor\n",
    "            model (str): Name of the model\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            model_full_name (str): Full name of the CAMS model among:\n",
    "            - 'cams-global-atmospheric-composition-forecasts' \n",
    "            - 'cams-global-reanalysis-eac4-monthly'\n",
    "            sensor_type (str): Sensor type\n",
    "    \"\"\"\n",
    "\n",
    "    if ((sensor == 'tropomi' and model == 'cams' and model_full_name == 'cams-global-atmospheric-composition-forecasts') or \n",
    "        (sensor == 'iasi' and sensor_type == 'L2' and model == 'cams' and model_full_name == 'cams-global-atmospheric-composition-forecasts') or\n",
    "        (sensor == 'iasi' and sensor_type == 'L3' and model == 'cams' and model_full_name == 'cams-global-reanalysis-eac4-monthly') or\n",
    "        (sensor == 'gome' and sensor_type == 'L2' and model == 'cams' and model_full_name == 'cams-global-atmospheric-composition-forecasts') or\n",
    "        (sensor == 'gome' and sensor_type == 'L3' and model == 'cams' and model_full_name == 'cams-global-reanalysis-eac4-monthly')):\n",
    "\n",
    "        if (model_full_name != 'cams-global-atmospheric-composition-forecasts' and\n",
    "            model_full_name != 'cams-global-reanalysis-eac4-monthly'):\n",
    "\n",
    "            print('ERROR: The model is not supported.')\n",
    "            print('The models that are currently supported are:')\n",
    "            print('- cams-global-atmospheric-composition-forecasts')\n",
    "            print('- cams-global-reanalysis-eac4-monthly')\n",
    "            raise KeyboardInterrupt()\n",
    "\n",
    "        else:\n",
    "            \n",
    "            tropomi_component_nom = ['NO2', 'CO', 'O3', 'SO2']\n",
    "            iasi_L2_component_nom = ['O3', 'CO']\n",
    "            iasi_L3_component_nom = ['CO', 'O3']\n",
    "            gome_L2_component_nom = ['NO2', 'O3', 'HCHO']\n",
    "            gome_L3_component_nom = ['NO2']\n",
    "\n",
    "            if ((sensor == 'tropomi' and component_nom not in tropomi_component_nom) or\n",
    "                (sensor == 'iasi' and sensor_type == 'L2' and component_nom not in iasi_L2_component_nom) or\n",
    "                (sensor == 'iasi' and sensor_type == 'L3' and component_nom not in iasi_L3_component_nom) or\n",
    "                (sensor == 'gome' and sensor_type == 'L2' and component_nom not in gome_L2_component_nom) or\n",
    "                (sensor == 'gome' and sensor_type == 'L3' and component_nom not in gome_L3_component_nom)):\n",
    "\n",
    "                print(f'ERROR: This specific component cannot be retrieved by the sensor {sensor.upper()} ({sensor_type}).')\n",
    "                raise KeyboardInterrupt()\n",
    "\n",
    "            else:\n",
    "\n",
    "                print('The comparison is possible and will start now.')\n",
    "    else:\n",
    "\n",
    "        print('Currently, it is possible to compare:')\n",
    "        print('1 - Forecast data from CAMS model and L2 data from TROPOMI, IASI and GOME-2 sensors.')\n",
    "        print('2 - Reanalysis data from CAMS model and L3 monthly data from IASI and GOME-2 sensors.')\n",
    "\n",
    "        raise KeyboardInterrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_table(sensor, component_nom, sensor_type):\n",
    "\n",
    "    \"\"\" Create table with information about the components (molecular weight, full name in different datasets)\n",
    "\n",
    "        Args:\n",
    "            sensor (str): Name of the sensor\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            sensor_type (str): Sensor type\n",
    "\n",
    "        Returns:\n",
    "            component (str): Component name\n",
    "            component_mol_weight (float): Component molecular weight\n",
    "            component_sensor_product (str): Component product name in TROPOMI, IASI or GOME-2 database\n",
    "            sensor_column (str): Component column name in TROPOMI, IASI or GOME-2 database\n",
    "    \"\"\"\n",
    "\n",
    "    sensor_product_type = None\n",
    "\n",
    "    component_nom_col = ['NO2', 'CO', 'O3', 'SO2', 'CH4', 'HCHO', 'NH3']\n",
    "\n",
    "    component_col = ['nitrogen_dioxide', 'carbon_monoxide', 'ozone', 'sulfur_dioxide', 'methane', 'formaldehyde', 'ammonia']\n",
    "    component_mol_weight_col = [46.005, 28.01, 48, 64.066, 16.04, 30.031, 17.031]\n",
    "    component_tropomi_product_col = ['L2__NO2___', 'L2__CO____', 'L2__O3____', 'L2__SO2___', 'L2__CH4___', '-', '-']\n",
    "    component_tropomi_column_col = ['nitrogendioxide_tropospheric_column', \n",
    "                                    'carbonmonoxide_total_column', \n",
    "                                    'ozone_total_vertical_column', \n",
    "                                    'sulfurdioxide_total_vertical_column',\n",
    "                                    'methane_tropospheric_column',\n",
    "                                    '-',\n",
    "                                    '-'\n",
    "                                    ]\n",
    "    component_iasi_L3_column_col = ['-', 'COgridDAY', 'O3gridDAY', '-', '-', '-', 'NH3gridDAY']\n",
    "    component_iasi_L2_column_col = ['-', 'CO_total_column', 'O3_total_column', '-', '-', '-', '']\n",
    "    component_gome_L3_column_col = ['NO2trop', '-', 'O3total', '-', '-', 'HCHOtotal', '-']\n",
    "    component_gome_L2_column_col = component_gome_L3_column_col\n",
    "\n",
    "    rows = {'Nomenclature': component_nom_col, \n",
    "            'Weight': component_mol_weight_col,\n",
    "            'Component': component_col, \n",
    "            'TROPOMI_L2_product': component_tropomi_product_col,\n",
    "            'TROPOMI_L2_column': component_tropomi_column_col,\n",
    "            'IASI_L3_column': component_iasi_L3_column_col,\n",
    "            'IASI_L2_column': component_iasi_L2_column_col,\n",
    "            'GOME_L3_column': component_gome_L3_column_col,\n",
    "            'GOME_L2_column': component_gome_L2_column_col}\n",
    "\n",
    "    components_table = pd.DataFrame(rows)\n",
    "\n",
    "    component = components_table['Component'].loc[components_table['Nomenclature'] == component_nom].iloc[0]\n",
    "    component_mol_weight = components_table['Weight'].loc[components_table['Nomenclature'] == component_nom].iloc[0]\n",
    "    \n",
    "    if sensor == 'tropomi':\n",
    "        sensor_product_type = components_table['TROPOMI_L2_product'].loc[components_table['Nomenclature'] == component_nom].iloc[0]\n",
    "\n",
    "    sensor_column = components_table[sensor.upper() + '_' + sensor_type + '_column'].loc[components_table['Nomenclature'] == component_nom].iloc[0]\n",
    "\n",
    "    return component, component_mol_weight, sensor_product_type, sensor_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_folders(model, sensor, component_nom, sensor_type):\n",
    "\n",
    "    \"\"\" Generate folders to download the datasets if they do not exist \n",
    "\n",
    "        Args:\n",
    "            model (str): Name of the model\n",
    "            sensor (str): Name of the sensor\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            sensor_type (str): Sensor type\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = os.path.join('/', '/'.join(os.getcwd().split('/')[1:3]), 'adc-toolbox', os.path.relpath('data/' + model + '/' + component_nom))\n",
    "    \n",
    "    if sensor_type == 'L3':\n",
    "        sensor_path = os.path.join('/', '/'.join(os.getcwd().split('/')[1:3]), 'adc-toolbox', os.path.relpath('data/' + sensor + '/' + component_nom + '/monthly/'))\n",
    "        \n",
    "    else:\n",
    "        sensor_path = os.path.join('/', '/'.join(os.getcwd().split('/')[1:3]), 'adc-toolbox', os.path.relpath('data/' + sensor + '/' + component_nom))\n",
    "\n",
    "    paths = [model_path, sensor_path]\n",
    "\n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_period(start_date, end_date, sensor, sensor_type):\n",
    "\n",
    "    \"\"\" Give list or tuple with dates that will be used to download the datasets\n",
    "\n",
    "        Args:\n",
    "            start_date (str): Query start date\n",
    "            end_date (str): Query end date\n",
    "            sensor (str): Name of the sensor\n",
    "            sensor_type (str): Sensor type\n",
    "\n",
    "        Returns:\n",
    "            dates (list or tuple): Query dates\n",
    "    \"\"\"\n",
    "\n",
    "    print('SEARCH PERIOD')\n",
    "\n",
    "    start_date_dt = dt.datetime(int(start_date.split('-')[0]), int(start_date.split('-')[1]), int(start_date.split('-')[2]), 0, 0, 0)\n",
    "    end_date_dt = dt.datetime(int(end_date.split('-')[0]), int(end_date.split('-')[1]), int(end_date.split('-')[2]), 0, 0, 0)\n",
    "    range_dt = pd.date_range(start_date_dt, end_date_dt)\n",
    "\n",
    "    if (sensor == 'gome' and sensor_type == 'L2') or (sensor == 'iasi' and sensor_type == 'L2'):\n",
    "        dates = tuple(np.unique([date.strftime('%Y-%m-%d') for date in range_dt]))\n",
    "        print(f'- In days: {dates}')\n",
    "\n",
    "    elif (sensor == 'gome' and sensor_type == 'L3') or (sensor == 'iasi' and sensor_type == 'L3'):\n",
    "        dates = tuple(np.unique([date.strftime('%Y-%m') for date in range_dt]))\n",
    "        print(f'- In months: {dates}')\n",
    "\n",
    "    elif sensor == 'tropomi':\n",
    "        range_dt_initial = range_dt\n",
    "        range_dt_final = range_dt_initial + dt.timedelta(hours = 23)\n",
    "        dates = list(zip([date.strftime('%Y-%m-%dT%H:%M:%SZ') for date in range_dt_initial], \n",
    "                        [date.strftime('%Y-%m-%dT%H:%M:%SZ') for date in range_dt_final]))\n",
    "        print(f'- In days: {dates}')\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bbox(lon_min, lat_min, lon_max, lat_max):\n",
    "\n",
    "    \"\"\" Generate bounding box from coordinates\n",
    "        \n",
    "        Args:\n",
    "            lon_min (float): Minimum longitude\n",
    "            lat_min (float): Minimum latitude\n",
    "            lon_max (float): Maximum longitude\n",
    "            lat_max (float): Maximum latitude\n",
    "            \n",
    "        Returns:\n",
    "            bbox (arr): Query bounding box\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    bbox = ((lon_min, lat_min), (lon_max, lat_max))\n",
    "\n",
    "    print('SEARCH BOUNDING BOX')\n",
    "    print(f'Latitudes: from {lat_min} to {lat_max}')\n",
    "    print(f'Longitudes: from {lon_min} to {lon_max}')\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_period(sensor, sensor_type, dates, component_nom, *args):\n",
    "\n",
    "    \"\"\" Remove dates if the folders where the dataset had to be downloaded are empty (dataset not available)\n",
    "        \n",
    "        Args:\n",
    "            sensor (str): Name of the sensor\n",
    "            sensor_type (str): Sensor type\n",
    "            dates (list): Query dates\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            *args: satellites\n",
    "            \n",
    "        Returns:\n",
    "            dates (list or tuple): Available dates\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    dates_to_delete = []\n",
    "\n",
    "    for date in dates:\n",
    "        \n",
    "        if sensor_type == 'L3':\n",
    "            output_path = os.path.join('/', '/'.join(os.getcwd().split('/')[1:3]), 'adc-toolbox', os.path.relpath('data/' + sensor + '/' + component_nom + '/monthly/' + date))\n",
    "\n",
    "        elif sensor_type == 'L2':\n",
    "            output_path = os.path.join('/', '/'.join(os.getcwd().split('/')[1:3]), 'adc-toolbox', os.path.relpath('data/' + sensor + '/' + component_nom + '/' + date))\n",
    "\n",
    "        if not os.listdir(output_path):\n",
    "            os.rmdir(output_path)\n",
    "            dates_to_delete.append(date)\n",
    "\n",
    "        if sensor_type == 'L2' and sensor == 'gome':\n",
    "            for satellite in satellites:\n",
    "                if not os.listdir(output_path + '/' + satellite):\n",
    "                    os.rmdir(output_path+ '/' + satellite)\n",
    "\n",
    "    dates_to_keep = np.setdiff1d(dates, np.array(dates_to_delete))\n",
    "    dates = tuple(dates_to_keep)\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_download(sensor, sensor_type, component_nom, dates, *args):\n",
    "\n",
    "    \"\"\" Download sensor datasets\n",
    "\n",
    "        Args:\n",
    "            sensor (str): Name of the sensor\n",
    "            sensor_type (str): Sensor type\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            dates (list or tuple): Available dates\n",
    "            *args: bbox, satellites or product_type\n",
    "\n",
    "        Returns:\n",
    "            dates (list): Available dates\n",
    "    \"\"\" \n",
    "\n",
    "    print('RESULTS')\n",
    "    \n",
    "    if sensor == 'tropomi':\n",
    "\n",
    "        for date in dates:\n",
    "\n",
    "            print(f'For {date}:')\n",
    "            input_type = 'Query'\n",
    "            TROPOMI_download(input_type, bbox, date, product_type, component_nom)\n",
    "\n",
    "    elif sensor == 'iasi' or sensor == 'gome':\n",
    "        \n",
    "        for date in dates:\n",
    "            \n",
    "            print(f'For {date}:')\n",
    "\n",
    "            for satellite in satellites:\n",
    "\n",
    "                if sensor == 'iasi' and sensor_type == 'L2':\n",
    "                    IASI_L2_download(component_nom, date, satellite)\n",
    "\n",
    "                elif sensor == 'iasi' and sensor_type == 'L3':\n",
    "                    IASI_L3_download(component_nom, date, satellite)\n",
    "        \n",
    "                elif sensor == 'gome' and sensor_type == 'L2':\n",
    "                    GOME_L2_download(component_nom, date, satellite)\n",
    "\n",
    "                elif sensor == 'gome' and sensor_type == 'L3':\n",
    "                    GOME_L3_download(component_nom, date, satellite)\n",
    "\n",
    "        dates = available_period(sensor, sensor_type, dates, component_nom, satellites)\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_read(sensor, sensor_type, sensor_column, component_nom, dates, *args):\n",
    "\n",
    "    \"\"\" Read sensor datasets as xarray dataset objects\n",
    "\n",
    "        Args:\n",
    "            sensor (str): Name of the sensor\n",
    "            sensor_type (str): Sensor type\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            dates (list or tuple): Available dates\n",
    "            *args: satellites, lat_res, lon_res\n",
    "\n",
    "        Returns:\n",
    "            sensor_ds (xarray): sensor dataset in xarray format\n",
    "            support_input_ds (xarray): TROPOMI dataset that contains support input data in xarray format\n",
    "            support_details_ds (xarray): TROPOMI dataset that contains support details data in xarray format\n",
    "    \"\"\" \n",
    "    \n",
    "    support_input_ds = None\n",
    "    support_details_ds = None\n",
    "\n",
    "    if dates:\n",
    "\n",
    "        if sensor == 'tropomi':\n",
    "            sensor_ds, support_input_ds, support_details_ds = TROPOMI_read(dates, component_nom, sensor_column)\n",
    "        \n",
    "        elif sensor == 'iasi' and sensor_type == 'L2':\n",
    "            sensor_ds = IASI_L2_read(component_nom, sensor_column, dates, lat_res, lon_res)\n",
    "\n",
    "        elif sensor == 'iasi' and sensor_type == 'L3':\n",
    "            sensor_ds = IASI_L3_read(component_nom, sensor_column, dates, lat_res, lon_res)\n",
    "\n",
    "        elif sensor == 'gome' and sensor_type == 'L2':\n",
    "            sensor_ds = GOME_L2_read(component_nom, dates, lat_res, lon_res)\n",
    "\n",
    "        elif sensor == 'gome' and sensor_type == 'L3':\n",
    "            sensor_ds = GOME_L3_read(component_nom, sensor_column, dates, lat_res, lon_res)\n",
    "\n",
    "    else:\n",
    "        print('The datasets could not be downloaded for the dates that were queried.')\n",
    "        \n",
    "    return sensor_ds, support_input_ds, support_details_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_convert_units(sensor_ds, sensor, component):\n",
    "\n",
    "    \"\"\" Convert the units of the sensor dataset for any component from mol/m2 to molecules/cm2\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): sensor dataset in xarray format (TROPOMI, IASI or GOME-2)\n",
    "            sensor (str): Name of the sensor\n",
    "            component (str): Component name\n",
    "            \n",
    "        Returns:\n",
    "            sensor_ds (xarray): sensor dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    if sensor == 'tropomi':\n",
    "        \n",
    "        if sensor_ds['sensor_column'].units == 'mol m-2':\n",
    "\n",
    "            sensor_ds['sensor_column'] = sensor_ds['sensor_column'] * 6.02214*10**19\n",
    "            sensor_ds['sensor_column'] = sensor_ds['sensor_column'].assign_attrs({'units': 'molec cm-2'})\n",
    "            print('The sensor component units have been converted from mol cm-2 to molec cm-2.')\n",
    "            \n",
    "            if 'apriori_profile' in list(sensor_ds.keys()):\n",
    "                sensor_ds['apriori_profile'] = sensor_ds['apriori_profile'] * 6.02214*10**19\n",
    "\n",
    "            if sensor_ds['sensor_column'].units == 'molec cm-2' and component == 'ozone':\n",
    "                sensor_ds['sensor_column'] = sensor_ds['sensor_column'] / (2.69*10**16)\n",
    "                sensor_ds['sensor_column'] = sensor_ds['sensor_column'].assign_attrs({'units': 'DU'})\n",
    "                print('The sensor component units have been converted from molec cm-2 to DU.')\n",
    "\n",
    "                if 'apriori_profile' in list(sensor_ds.keys()):\n",
    "                    sensor_ds['apriori_profile'] = sensor_ds['apriori_profile'] / (2.69*10**16)\n",
    "\n",
    "    elif sensor == 'iasi':\n",
    "        \n",
    "        if sensor_ds.units == 'mol m-2':\n",
    "\n",
    "            sensor_ds = sensor_ds * 6.02214*10**19\n",
    "            sensor_ds = sensor_ds.assign_attrs({'units': 'molec cm-2'})\n",
    "            print('The sensor component units have been converted from mol cm-2 to molec cm-2.')\n",
    "\n",
    "        if sensor_ds.units == 'molec cm-2' and component == 'ozone':\n",
    "            sensor_ds = sensor_ds / (2.69*10**16)\n",
    "            sensor_ds = sensor_ds.assign_attrs({'units': 'DU'})\n",
    "            print('The sensor component units have been converted from molec cm-2 to DU.')\n",
    "\n",
    "    return sensor_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_convert_units(model, model_ds, sensor, component_mol_weight, component, model_levels_df, \n",
    "                        start_date, end_date, component_nom, apply_kernels = False, \n",
    "                        CAMS_UID = None, CAMS_key = None):\n",
    "\n",
    "    \"\"\" Convert the units of the model dataset for any component from kg/kg or kg/m2 to molecules/cm2\n",
    "\n",
    "        Args:\n",
    "            model (str): Name of the model\n",
    "            model_ds (xarray): model dataset in xarray format (CAMS)\n",
    "            sensor (str): Name of the sensor\n",
    "            component_mol_weight (float): Component molecular weight\n",
    "            component (str): Component name\n",
    "            model_levels_df (dataframe): Table with 137 CAMS levels data\n",
    "            start_date (str): Query start date\n",
    "            end_date (str): Query end date\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            apply_kernels (bool): Apply (True) or not (False) the averaging kernels \n",
    "            CAMS_UID (str): ADS user ID\n",
    "            CAMS_key (str): ADS key\n",
    "            \n",
    "        Returns:\n",
    "            model_ds (xarray): model dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'cams':\n",
    "\n",
    "        if model_ds.component.units == 'kg kg**-1':\n",
    "\n",
    "            model_ds = CAMS_kg_kg_to_kg_m2(model_ds, model_levels_df, sensor, start_date, \n",
    "                                           end_date, component_nom, apply_kernels, CAMS_UID, CAMS_key)\n",
    "            units = 'kg m**-2'\n",
    "            model_ds['component'] = model_ds.component.assign_attrs({'units': units})\n",
    "            print('The model component units have been converted from kg kg**-1 to kg m**-2.')\n",
    "            \n",
    "        if model_ds.component.units == 'kg m**-2':\n",
    "\n",
    "            model_ds = CAMS_kg_m2_to_molecules_cm2(model_ds, component_mol_weight)\n",
    "            units = 'molec cm-2'\n",
    "            model_ds['component'] = model_ds.component.assign_attrs({'units': units})\n",
    "            print('The model component units have been converted from kg m**-2 to molec cm-2.')\n",
    "\n",
    "        if model_ds.component.units == 'molec cm-2' and component == 'ozone':\n",
    "\n",
    "            model_ds = CAMS_molecules_cm2_to_DU(model_ds)\n",
    "            units = 'DU'\n",
    "            model_ds['component'] = model_ds.component.assign_attrs({'units': units})\n",
    "            print('The model component units have been converted from molec cm-2 to DU.')\n",
    "           \n",
    "        else:\n",
    "            units = 'molec cm-2'\n",
    "\n",
    "    return model_ds, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(array, value):\n",
    "\n",
    "    \"\"\" Find index of the closest value in a 1D-array\n",
    "\n",
    "        Args:\n",
    "            array (arr): Array to find the nearest neighbour\n",
    "            value (float): Search value\n",
    "    \"\"\"\n",
    "\n",
    "    index = np.abs([x - value for x in array]).argmin(0)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_point(point, array):\n",
    "\n",
    "    \"\"\" Find pair the closest values in a 2D-array\n",
    "\n",
    "        Args:\n",
    "            array (arr): Array to find the nearest neighbour\n",
    "            point (tuple): Search coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    pair = array[cdist([point], array).argmin()]\n",
    "\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(dates):\n",
    "\n",
    "    \"\"\" Split dates array in pairs\n",
    "\n",
    "        Args:\n",
    "            dates (arr): All dates\n",
    "\n",
    "        Returns:\n",
    "            period (tuple): Divisible dates into pairs\n",
    "    \"\"\"\n",
    "\n",
    "    pair_element = iter(dates)\n",
    "    period = list(zip(pair_element, pair_element))\n",
    "\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(ds, bbox, sensor, component_nom, subset_type):\n",
    "\n",
    "    \"\"\" Subset any dataset (with latitude and longitude as coordinates) into desired bounding box.\n",
    "\n",
    "        Args:\n",
    "            ds (xarray): Dataset in xarray format\n",
    "            bbox (arr): Query bounding box\n",
    "    \n",
    "        Returns:\n",
    "            ds (xarray): Dataset in xarray format\n",
    "    \"\"\"\n",
    "\n",
    "    if sensor == 'tropomi' and subset_type == 'sensor_subset':\n",
    "\n",
    "        ds = TROPOMI_subset(ds, bbox, component_nom)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Get nearest longitude and latitude to bbox\n",
    "        lon_min_index = nearest_neighbour(ds.longitude.data, bbox[0][0])\n",
    "        lon_max_index = nearest_neighbour(ds.longitude.data, bbox[1][0])\n",
    "        lat_min_index = nearest_neighbour(ds.latitude.data, bbox[0][1])\n",
    "        lat_max_index = nearest_neighbour(ds.latitude.data, bbox[1][1])\n",
    "\n",
    "        # Define slices\n",
    "        slice_lat = slice(lat_min_index, lat_max_index + 1)\n",
    "        slice_lon = slice(lon_min_index, lon_max_index + 1)\n",
    "\n",
    "        # Set limits\n",
    "        ds = ds.isel(longitude = slice_lon, latitude = slice_lat)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(match_df, sensor, component_nom, time):\n",
    "\n",
    "    \"\"\" Prepare dataframe for match\n",
    "\n",
    "        Args:\n",
    "            match_df (dataframe): Dataframe used to apply averaging kernels\n",
    "            sensor (str): Name of the sensor\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            time (timestamp): Current time\n",
    "        \n",
    "        Returns:\n",
    "            match_df (dataframe): Dataframe used to apply averaging kernels\n",
    "    \"\"\"\n",
    "\n",
    "    if sensor == 'tropomi':\n",
    "\n",
    "        # Pass NaNs to data with qa_value under 0.5 (these values will be shown as transparent)\n",
    "        match_df.loc[match_df['qa_value'] <= 0.5, ['sensor_column', 'column_kernel']] = float('NaN')\n",
    "\n",
    "        # Drop levels\n",
    "        if component_nom == 'CO' or component_nom == 'SO2':\n",
    "            \n",
    "            match_df.index.names = ['corner', 'ground_pixel', 'layer', 'scanline']\n",
    "        \n",
    "        elif component_nom == 'O3':\n",
    "\n",
    "            match_df.index.names = ['corner', 'ground_pixel', 'layer', 'level', 'scanline']\n",
    "            \n",
    "        match_df = match_df.groupby(by = ['layer', 'scanline', 'ground_pixel', 'time', 'delta_time']).mean()\n",
    "        match_df = match_df.reset_index(level = ['layer', 'delta_time'])\n",
    "\n",
    "    elif sensor == 'iasi' or sensor == 'gome':\n",
    "\n",
    "        match_df = match_df.reset_index(level = ['latitude', 'longitude'])\n",
    "        \n",
    "        if sensor == 'gome' and sensor_type == 'L2':\n",
    "\n",
    "            year = time.astype('datetime64[D]').astype(str).split('-')[0]\n",
    "            month = time.astype('datetime64[D]').astype(str).split('-')[1]\n",
    "            day = time.astype('datetime64[D]').astype(str).split('-')[2]\n",
    "            match_df['delta_time'] = match_df['delta_time'].fillna(value = dt.datetime(int(year), \n",
    "                                                                                       int(month), \n",
    "                                                                                       int(day), \n",
    "                                                                                       12, 0, 0))\n",
    "        \n",
    "    return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_match_table(sensor_ds, model_ds, bbox, sensor, component_nom, apply_kernels = False):\n",
    "\n",
    "    \"\"\" Intermediate merge table with total column or partial column from both datasets, \n",
    "        the averaging kernels are applied if possible\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): sensor dataset in xarray format (TROPOMI, IASI or GOME-2)\n",
    "            model_ds (xarray): model dataset in xarray format (CAMS)\n",
    "            bbox (arr): Query bounding box\n",
    "            sensor (str): Name of the sensor\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            apply_kernels (bool): Apply (True) or not (False) the averaging kernels \n",
    "\n",
    "        Returns:\n",
    "            match_table (dataframe): Intermediate merge table with total column or partial column from both datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    match_table = pd.DataFrame()\n",
    "\n",
    "    if sensor == 'tropomi' and apply_kernels == True:\n",
    "\n",
    "        print('APPLICATION OF AVERAGING KERNELS')\n",
    "        print('For the application of the averaging kernels, it is necessary to calculate:')\n",
    "        print('1. Level pressures')\n",
    "        print('2. Column kernels')\n",
    "        print('The apriori profiles should be retrieved, but they are not necessary.')\n",
    "\n",
    "        # Calculate TM5 level pressures, column kernels and apriori profiles\n",
    "        print('DATA AVAILABILITY')\n",
    "        sensor_ds = TROPOMI_pressure(sensor_ds, component_nom, support_input_ds, support_details_ds)\n",
    "        sensor_ds = TROPOMI_column_kernel(sensor_ds, component_nom, support_details_ds)\n",
    "        sensor_ds = TROPOMI_apriori_profile(sensor_ds, component, support_details_ds)\n",
    "\n",
    "    for time in sensor_ds.time.values:\n",
    "        \n",
    "        # Print estimated time or month\n",
    "        if sensor_type == 'L2':\n",
    "            day = np.datetime64(time).astype('datetime64[D]')\n",
    "            print(f'FOR DATE: {day}')\n",
    "\n",
    "        elif sensor_type == 'L3':\n",
    "            month = np.datetime64(time).astype('datetime64[M]')\n",
    "            print(f'FOR MONTH: {month}')\n",
    "\n",
    "        # Reduce data to only one timestamp\n",
    "        model_ds_time = model_ds.sel(time = time)\n",
    "        sensor_ds_time = sensor_ds.sel(time = time)\n",
    "\n",
    "        # Subset sensor dataset\n",
    "        sensor_ds_time = subset(sensor_ds_time, bbox, sensor, component_nom, subset_type = 'sensor_subset')\n",
    "        \n",
    "        # Transform sensor data into dataframe and prepare it for merging it with the model data\n",
    "        match_df = sensor_ds_time.to_dataframe()\n",
    "        match_df = prepare_df(match_df, sensor, component_nom, time)\n",
    "        \n",
    "        if sensor == 'tropomi' and 'column_kernel' in list(sensor_ds.keys()) and apply_kernels == True:\n",
    "\n",
    "            match_df = TROPOMI_apply_kernels(match_df, model_ds_time, sensor_ds_time, component_nom)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if apply_kernels == True:\n",
    "                print('The application of the averaging kernels cannot take place because there is no data of the column kernels.')\n",
    "\n",
    "            if 'hybrid' in list(model_ds.coords):\n",
    "\n",
    "                print('The partial columns will be sumed up.')\n",
    "                print('The sum will be matched to the sensor data by nearest neighbours.')\n",
    "\n",
    "                model_ds_time = model_ds_time.component.sum(dim = 'hybrid', skipna = False)\n",
    "                model_times = model_ds_time.valid_time.data\n",
    "                \n",
    "                match_df['step_index'] = match_df.apply(lambda row: nearest_neighbour(model_times, row['delta_time']), axis = 1)\n",
    "                match_df['model_time'] = match_df.apply(lambda row: model_ds_time.valid_time[row['step_index']].values, axis = 1)\n",
    "                match_df['model_column'] = match_df.apply(lambda row: model_ds_time.sel(\n",
    "                                                                      latitude = row['latitude'], \n",
    "                                                                      longitude = row['longitude'],\n",
    "                                                                      method = 'nearest').isel(step = \n",
    "                                                                      int(row['step_index'])).values, \n",
    "                                                                      axis = 1)\n",
    "   \n",
    "                match_df = match_df.set_index('layer', append = True)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                print('The model dataset does not contain levels data.')\n",
    "                print('The model dataset will be merged with the sensor dataset by nearest neighbours.')\n",
    "\n",
    "                model_times = model_ds_time.valid_time.data\n",
    "\n",
    "                # Monthly data\n",
    "                if 'step' not in list(model_ds.dims):\n",
    "                    \n",
    "                    match_df['model_column'] = match_df.apply(lambda row: float(model_ds_time.sel(\n",
    "                                                                                latitude = row['latitude'], \n",
    "                                                                                longitude = row['longitude'],\n",
    "                                                                                method = 'nearest').component.values), \n",
    "                                                                                axis = 1)\n",
    "                # Hourly / Daily data\n",
    "                else:\n",
    "\n",
    "                    match_df['step_index'] = match_df.apply(lambda row: nearest_neighbour(model_times, row['delta_time']), axis = 1)\n",
    "                    match_df['model_column'] = match_df.apply(lambda row: float(model_ds_time.sel(\n",
    "                                                                                latitude = row['latitude'], \n",
    "                                                                                longitude = row['longitude'],\n",
    "                                                                                method = 'nearest').isel(step = \n",
    "                                                                                int(row['step_index'])).component.values), \n",
    "                                                                                axis = 1)\n",
    "\n",
    "        match_df = match_df[~match_df.index.duplicated()]\n",
    "        match_table = match_table.append(match_df)\n",
    "\n",
    "    return match_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_table(match_table, sensor_ds, model_ds, sensor, apply_kernels = False):\n",
    "\n",
    "    \"\"\" Final merge table with total column component data for each dataset, \n",
    "        their difference in each grid point are calculated\n",
    "\n",
    "        Args:\n",
    "            match_table (dataframe): Intermediate merge table with total column or partial column from both datasets\n",
    "            sensor_ds (xarray): sensor dataset in xarray format (TROPOMI, IASI or GOME-2)\n",
    "            model_ds (xarray): model dataset in xarray format (CAMS)\n",
    "            apply_kernels (bool): Apply (True) or not (False) the averaging kernels\n",
    "            sensor (str): Name of the sensor\n",
    "        \n",
    "        Returns:\n",
    "            merge_table (dataframe): Merge table with datasets column data and their difference\n",
    "    \"\"\"\n",
    "\n",
    "    merge_table = []\n",
    "\n",
    "    if 'hybrid' in list(model_ds.coords):\n",
    "\n",
    "        for time in sensor_ds.time.values:\n",
    "\n",
    "            match_ds = match_table.query('time == @time').to_xarray()\n",
    "\n",
    "            # Read latitudes and longitudes from data array\n",
    "            latitude = match_ds.sel(time = time).latitude.mean(dim = 'layer')\n",
    "            longitude = match_ds.sel(time = time).longitude.mean(dim = 'layer')\n",
    "\n",
    "            # Get sum of CAMS data of each layer to get column data\n",
    "            if 'column_kernel' in list(match_ds.keys()) and apply_kernels == True:\n",
    "                model_final_ds_time = match_ds.sel(time = time).model_column.sum(dim = 'layer', skipna = False).astype(float)\n",
    "\n",
    "            else:\n",
    "                model_final_ds_time = match_ds.sel(time = time).model_column.mean(dim = 'layer', skipna = False).astype(float)\n",
    "\n",
    "            model_final_ds_time = model_final_ds_time.assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "            # Get mean of TROPOMI data of each layer (it must be equal)\n",
    "            sensor_final_ds_time = match_ds.sensor_column.sel(time = time).mean(dim = 'layer', skipna = False).astype(float)\n",
    "            sensor_final_ds_time = sensor_final_ds_time.assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "            merge_ds_time = xr.merge([model_final_ds_time, sensor_final_ds_time])\n",
    "            merge_ds_time['difference'] = merge_ds_time.sensor_column - merge_ds_time.model_column\n",
    "            merge_table.append(merge_ds_time.to_dataframe())\n",
    "\n",
    "        merge_table = pd.concat(merge_table)\n",
    "\n",
    "    else:\n",
    "\n",
    "        merge_table = match_table\n",
    "        merge_table['difference'] = merge_table['sensor_column'] - merge_table['model_column']\n",
    "\n",
    "    # Organize dataset for visualization\n",
    "    if sensor == 'tropomi':\n",
    "        merge_table = merge_table.reset_index().set_index(['scanline', 'ground_pixel', 'time'])\n",
    "        merge_table = merge_table[['latitude', 'longitude', 'model_column', 'sensor_column', 'difference']]\n",
    "\n",
    "    else:\n",
    "        merge_table = merge_table.reset_index().set_index(['latitude', 'longitude', 'time'])\n",
    "        merge_table = merge_table[['model_column', 'sensor_column', 'difference']]\n",
    "    \n",
    "    return merge_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_period(sensor_ds, sensor):\n",
    "\n",
    "    \"\"\" Define plot period\n",
    "\n",
    "        Args:\n",
    "            sensor_ds (xarray): sensor dataset in xarray format (TROPOMI, IASI or GOME-2)\n",
    "\n",
    "        Returns:\n",
    "            plot_dates (arr): Plot dates\n",
    "    \"\"\"\n",
    "\n",
    "    period_answer = input('Do you want to visualize the plots for specific dates? Press Enter for Yes or write No:')\n",
    "    plot_dates = []\n",
    "\n",
    "    if period_answer == 'No' or period_answer == 'no':\n",
    "        plot_dates = sensor_ds.time.values\n",
    "    \n",
    "    else:\n",
    "        if sensor == 'tropomi':\n",
    "            options_df = pd.DataFrame({'Date': sensor_ds.time.values})\n",
    "        \n",
    "        elif sensor == 'iasi' or sensor == 'gome':\n",
    "            options_df = pd.DataFrame({'Date': sensor_ds.time.values.astype('datetime64[M]')})\n",
    "\n",
    "        for index, row in options_df.iterrows():\n",
    "            date_answer = input('Do you want to show the plots for ' + str(row['Date']) + '? Press Enter for Yes or write No:') \n",
    "            if date_answer == 'No' or date_answer == 'no':\n",
    "                pass\n",
    "            else:\n",
    "                plot_dates = np.append(plot_dates, row['Date'])\n",
    "\n",
    "    print('The plots will be shown for the following dates:')\n",
    "    if sensor == 'tropomi':\n",
    "        print(plot_dates)\n",
    "    \n",
    "    elif sensor == 'iasi' or sensor == 'gome':\n",
    "        print(plot_dates.astype('datetime64[M]'))\n",
    "\n",
    "    return plot_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_extent(bbox):\n",
    "\n",
    "    \"\"\" Define plot extent\n",
    "\n",
    "        Args:\n",
    "            bbox (arr): Query bounding box\n",
    "\n",
    "        Returns:\n",
    "            plot_bbox (arr): Plot bounding box\n",
    "    \"\"\"\n",
    "\n",
    "    extent_answer = input(f'Do you want to visualize the plots for a specific extent? Press Enter for Yes or write No (default {bbox}):')\n",
    "\n",
    "    if extent_answer == 'No' or extent_answer == 'no':\n",
    "        plot_bbox = ((bbox[0][0], bbox[0][1]), (bbox[1][0], bbox[1][1]))\n",
    "\n",
    "    else:\n",
    "        # Define minimum longitude\n",
    "        plot_lon_min = float(input('Write value of minimum longitude: '))\n",
    "        while (plot_lon_min < bbox[0][0]) or (plot_lon_min > bbox[1][0]):\n",
    "            print(f'ERROR: Longitude must be between {bbox[0][0]} and {bbox[1][0]}.')\n",
    "            plot_lon_min = float(input('Write value of minimum longitude (again): '))\n",
    "\n",
    "        # Define maximum longitude\n",
    "        plot_lon_max = float(input('Write value of maximum longitude: '))\n",
    "        while (plot_lon_max < bbox[0][0]) or (plot_lon_max > bbox[1][0]) or (plot_lon_max <= plot_lon_min):\n",
    "            print(f'ERROR: Longitude must be between {bbox[0][0]} and {bbox[1][0]} and be higher than the minimum {plot_lon_min}.')\n",
    "            plot_lon_max = float(input('Write value of maximum longitude (again): '))\n",
    "\n",
    "        # Define minimum latitude\n",
    "        plot_lat_min = float(input('Write value of minimum latitude: '))\n",
    "        while (plot_lat_min < bbox[0][1]) or (plot_lat_min > bbox[1][1]):\n",
    "            print(f'ERROR: Latitude must be between {bbox[0][1]} and {bbox[1][1]}.')\n",
    "            plot_lat_min = float(input('Write value of minimum latitude (again): '))\n",
    "\n",
    "        # Define maximum latitude\n",
    "        plot_lat_max = float(input('Write value of maximum latitude: '))\n",
    "        while (plot_lat_max < bbox[0][1]) or (plot_lat_max > bbox[1][1]) or (plot_lat_max <= plot_lat_min):\n",
    "            print(f'ERROR: Latitude must be between {bbox[0][1]} and {bbox[1][1]} and be higher than the minimum {plot_lat_min}.')\n",
    "            plot_lat_max = float(input('Write value of maximum latitude (again): '))\n",
    "\n",
    "        # Define plot bbox\n",
    "        plot_bbox = ((plot_lon_min, plot_lat_min), (plot_lon_max, plot_lat_max))\n",
    "\n",
    "    print('The plots will be shown for the following spatial extent: ')\n",
    "    print(plot_bbox)\n",
    "    \n",
    "    return plot_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorbar_range(range_type, merge, array, \n",
    "                   max_all, min_all, max_all_diff, min_all_diff,\n",
    "                   vmin_manual = None, vmax_manual = None):\n",
    "\n",
    "    \"\"\" Define colorbar range\n",
    "\n",
    "        Args:\n",
    "            range_type (str): Range type for colorbar:\n",
    "            -  'original': Show original values in range\n",
    "            -  'equal': Show same scale in range\n",
    "            merge (xarray): Merge result for a specific time\n",
    "            array (xarray): Component for a specific time and model/sensor\n",
    "\n",
    "        Returns:\n",
    "            vmin, vmax (float): Limits of color bar\n",
    "    \"\"\"\n",
    "    \n",
    "    # The colorbar for difference will be defined\n",
    "    if np.array_equal(array, merge.difference, equal_nan = True) == True:\n",
    "        \n",
    "        if np.abs(max_all_diff) >= np.abs(min_all_diff):\n",
    "            \n",
    "            vmin = -np.abs(max_all_diff)\n",
    "            vmax = np.abs(max_all_diff)\n",
    "\n",
    "        elif np.abs(max_all_diff) < np.abs(min_all_diff):\n",
    "            \n",
    "            vmin = -np.abs(min_all_diff)\n",
    "            vmax = np.abs(min_all_diff)\n",
    "\n",
    "    # The colorbar will show the original range\n",
    "    elif range_type == 'original':\n",
    "      \n",
    "        vmin = np.nanmin(array)\n",
    "        vmax = np.nanmax(array)\n",
    "\n",
    "    # The colorbar will be in the same scale for both datasets\n",
    "    elif range_type == 'equal':\n",
    "       \n",
    "        vmin = min_all\n",
    "        vmax = max_all\n",
    "    \n",
    "    # The colorbar will be in the scale given by the user\n",
    "    elif range_type == 'manual':\n",
    "       \n",
    "        if vmin_manual == None or vmax_manual == None:\n",
    "            print('ERROR: vmin_manual and vmax_manual have to be defined and cannot be None.')\n",
    "            raise KeyboardInterrupt()\n",
    "            \n",
    "        else:\n",
    "            vmin = vmin_manual\n",
    "            vmax = vmax_manual\n",
    "\n",
    "    return vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pcolormesh(fig, axs, data_array, longitude, latitude, projection, color_scale, \n",
    "                         pad, long_name, units_name, vmin, vmax, lonmin, lonmax, latmin, latmax):\n",
    "    \n",
    "    \"\"\" Visualize two datasets side by side\n",
    "\n",
    "        Args:\n",
    "            fig: Figure\n",
    "            axs: Axes of figure\n",
    "            data_array (xarray): Variable values to plot - It must be 2-dimensional\n",
    "            longitude (arr): Longitudes within data_array\n",
    "            latitude (arr): Latitudes within data_array\n",
    "            projection: Geographical projection\n",
    "            color_scale (str): Color scale for the color bar\n",
    "            pad (float): Padding for the subtitles\n",
    "            long_name (str): Plot name\n",
    "            units_name (str): Component name and units\n",
    "            vmin, vmax (float): Limits of color bar\n",
    "            lonmin, lonmax, latmin, latmax (float): Limits of longitude and latitude values\n",
    "    \"\"\"\n",
    "\n",
    "    palette = copy(plt.get_cmap(color_scale))\n",
    "    palette.set_under(alpha = 0)\n",
    "    axs.clear()\n",
    "    im_ind = axs.pcolormesh(\n",
    "                        longitude, latitude, data_array, \n",
    "                        cmap = palette, \n",
    "                        transform = ccrs.PlateCarree(),\n",
    "                        vmin = vmin,\n",
    "                        vmax = vmax,\n",
    "                        norm = colors.Normalize(vmin = vmin, vmax = vmax),\n",
    "                        shading = 'auto'\n",
    "                        )\n",
    "                        \n",
    "    axs.add_feature(cfeature.BORDERS, edgecolor = 'black', linewidth = 1)\n",
    "    axs.add_feature(cfeature.COASTLINE, edgecolor = 'black', linewidth = 1)\n",
    "    axs.gridlines()\n",
    "\n",
    "    if projection == ccrs.PlateCarree():\n",
    "        \n",
    "        axs.set_extent([lonmin, lonmax, latmin, latmax], ccrs.PlateCarree())\n",
    "        gl = axs.gridlines(draw_labels = True, linestyle = '--')\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xformatter = LONGITUDE_FORMATTER\n",
    "        gl.yformatter = LATITUDE_FORMATTER\n",
    "        gl.xlabel_style = {'size': 16}\n",
    "        gl.ylabel_style = {'size': 16}\n",
    "\n",
    "    axs.set_title(long_name, fontsize = 18, pad = pad)\n",
    "    axs.tick_params(labelsize = 14)\n",
    "    \n",
    "    if distribution_type != 'animated':\n",
    "        \n",
    "        cbr = fig.colorbar(im_ind, ax = axs, extend = 'both', orientation = 'horizontal', \n",
    "                           fraction = 0.05, pad = 0.15)   \n",
    "        cbr.set_label(units_name, fontsize = 16)\n",
    "        cbr.ax.tick_params(labelsize = 14)\n",
    "        cbr.ax.xaxis.get_offset_text().set_fontsize(14)\n",
    "      \n",
    "    return im_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maps(fig, axs, merge, range_type, sensor, model, sensor_type, model_type, projection, pad, \n",
    "                units_name, plot_bbox, color_scale, max_all, min_all, min_all_diff, max_all_diff,\n",
    "                vmin_manual = None, vmax_manual = None):\n",
    "\n",
    "    # First plot - CAMS \n",
    "    array = merge['model_column']\n",
    "    vmin, vmax = colorbar_range(range_type, merge, array, max_all, min_all, max_all_diff, min_all_diff, vmin_manual, vmax_manual)\n",
    "    long_name = model.upper() + ' (' + model_type + ')'\n",
    "    im1 = visualize_pcolormesh(\n",
    "                              fig = fig, axs = axs[0], \n",
    "                              data_array = array,\n",
    "                              longitude = array.longitude,\n",
    "                              latitude = array.latitude,\n",
    "                              projection = projection,\n",
    "                              color_scale = color_scale,\n",
    "                              pad = pad,\n",
    "                              long_name = long_name,\n",
    "                              units_name = units_name,\n",
    "                              vmin = vmin, \n",
    "                              vmax = vmax, \n",
    "                              lonmin = plot_bbox[0][0],\n",
    "                              lonmax = plot_bbox[1][0],\n",
    "                              latmin = plot_bbox[0][1],\n",
    "                              latmax = plot_bbox[1][1]\n",
    "                              )\n",
    "\n",
    "    # Second plot - TROPOMI, IASI or GOME-2\n",
    "    array = merge['sensor_column']\n",
    "    vmin, vmax = colorbar_range(range_type, merge, array, max_all, min_all, max_all_diff, min_all_diff, vmin_manual, vmax_manual)\n",
    "    long_name = sensor.upper() + ' (' + sensor_type + ')'\n",
    "    im2 = visualize_pcolormesh(\n",
    "                              fig = fig, axs = axs[1],\n",
    "                              data_array = array,\n",
    "                              longitude = array.longitude,\n",
    "                              latitude = array.latitude,\n",
    "                              projection = projection,\n",
    "                              color_scale = color_scale,\n",
    "                              pad = pad,\n",
    "                              long_name = long_name,\n",
    "                              units_name = units_name,\n",
    "                              vmin = vmin,  \n",
    "                              vmax = vmax, \n",
    "                              lonmin = plot_bbox[0][0],\n",
    "                              lonmax = plot_bbox[1][0],\n",
    "                              latmin = plot_bbox[0][1],\n",
    "                              latmax = plot_bbox[1][1]\n",
    "                              )\n",
    "\n",
    "    # Third plot - Differences\n",
    "    array = merge.difference\n",
    "    vmin, vmax = colorbar_range(range_type, merge, array, max_all, min_all, max_all_diff, min_all_diff, vmin_manual, vmax_manual)\n",
    "    long_name = 'Differences plot'\n",
    "    im3 = visualize_pcolormesh(\n",
    "                              fig = fig, axs = axs[2],\n",
    "                              data_array = array,\n",
    "                              longitude = array.longitude,\n",
    "                              latitude = array.latitude,\n",
    "                              projection = projection,\n",
    "                              color_scale = color_scale,\n",
    "                              pad = pad,\n",
    "                              long_name = long_name,\n",
    "                              units_name = units_name,\n",
    "                              vmin = vmin,\n",
    "                              vmax = vmax,\n",
    "                              lonmin = plot_bbox[0][0],\n",
    "                              lonmax = plot_bbox[1][0],\n",
    "                              latmin = plot_bbox[0][1],\n",
    "                              latmax = plot_bbox[1][1]\n",
    "                              )\n",
    "    \n",
    "    im = [im1, im2, im3]\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_absolute_limits(merge_table_bbox, vmin_manual, vmax_manual):\n",
    "\n",
    "    \"\"\" Define absolute minimum and maximum within model and sensor datasets.\n",
    "        Gets manual minimum and maximum or calculates it\n",
    "\n",
    "        Args:\n",
    "            merge_table_bbox (dataframe): Merge table for plot bbox\n",
    "            vmin_manual (float): Input vmin by user\n",
    "            vmax_manual (float): Input vmax by user\n",
    "\n",
    "        Returns\n",
    "            min_all (float): Absolute vmin\n",
    "            max_all (float): Absolute vmax\n",
    "            min_all_diff (float): Absolute vmin for difference values\n",
    "            max_all_diff (float): Absolute vmax for difference values     \n",
    "    \"\"\"\n",
    "\n",
    "    # Define absolute minimum and maximum within model and sensor datasets\n",
    "    if vmin_manual == None and vmax_manual == None:\n",
    "        \n",
    "        min_model = np.nanmin(merge_table_bbox['model_column'])\n",
    "        max_model = np.nanmax(merge_table_bbox['model_column'])\n",
    "        min_sensor = np.nanmin(merge_table_bbox['sensor_column'])\n",
    "        max_sensor = np.nanmax(merge_table_bbox['sensor_column'])\n",
    "        max_all = max(max_sensor, max_model)\n",
    "        min_all = min(min_sensor, min_model)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if np.abs(vmax_manual) >= np.abs(vmin_manual):\n",
    "            \n",
    "            min_all = -np.abs(vmax_manual)\n",
    "            max_all = np.abs(vmax_manual)\n",
    "\n",
    "        elif np.abs(vmax_manual) < np.abs(vmin_manual):\n",
    "            \n",
    "            min_all = -np.abs(vmin_manual)\n",
    "            max_all = np.abs(vmin_manual)\n",
    "\n",
    "    # Define absolute minimum and maximum within difference\n",
    "    min_all_diff = np.nanmin(merge_table_bbox['difference'])\n",
    "    max_all_diff = np.nanmax(merge_table_bbox['difference'])\n",
    "\n",
    "    return min_all, max_all, min_all_diff, max_all_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_vs_sensor(model, sensor, component_nom, units, merge_table, plot_dates, plot_bbox, pad, y, \n",
    "                              model_type, sensor_type, range_type, distribution_type, projection,\n",
    "                              color_scale, vmin_manual = None, vmax_manual = None):\n",
    "\n",
    "    \"\"\" Plot model and sensor datasets in the study area for the selected dates, \n",
    "        along with a plot of the differences\n",
    "\n",
    "        Args:\n",
    "            model (str): Name of the model\n",
    "            sensor (str): Name of the sensor\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            units (str): Component units\n",
    "            merge_table (dataframe): Merge table with datasets column data and their difference\n",
    "            plot_dates (arr): Plot dates\n",
    "            plot_bbox (arr): Plot extent\n",
    "            pad (float): Padding for the subtitles\n",
    "            y (float): y-position of main title\n",
    "            model_type (str): Model type:\n",
    "            -  'Forecast'\n",
    "            -  'Reanalysis'\n",
    "            sensor_type (str): Sensor type\n",
    "            range_type (str): Range type for colorbar:\n",
    "            -  'original': Show original values in range\n",
    "            -  'equal': Show same scale in range\n",
    "            distribution_type (str): \n",
    "            -  'aggregated': Aggregate plots by time\n",
    "            -  'individual': Show individual plots\n",
    "            -  'animated: Show animation\n",
    "            projection: Geographical projection\n",
    "            color_scale (str): Name of color scale (e.g. coolwarm)\n",
    "            vmin_manual (float): Input vmin by user\n",
    "            vmax_manual (float): Input vmax by user\n",
    "    \"\"\"\n",
    "    \n",
    "    units_name = component_nom + ' (' + units + ')'\n",
    "\n",
    "    # Get min and max \n",
    "    merge_table_bbox = merge_table.query('longitude >= @plot_bbox[0][0] and longitude <= @plot_bbox[1][0] and latitude >= @plot_bbox[0][1] and latitude <= @plot_bbox[1][1]')\n",
    "    min_all, max_all, min_all_diff, max_all_diff = define_absolute_limits(merge_table_bbox, vmin_manual, vmax_manual)\n",
    "\n",
    "    if distribution_type == 'aggregated':\n",
    "            \n",
    "        merge = merge_table.to_xarray().mean(dim = 'time')\n",
    "        latitude = merge.latitude\n",
    "        longitude = merge.longitude\n",
    "        merge = merge.assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize = (20, 5), subplot_kw = {'projection': projection})\n",
    "        fig.set_facecolor('w')\n",
    "\n",
    "        im = create_maps(fig, axs, merge, range_type, sensor, model, sensor_type, model_type, \n",
    "                        projection, pad, units_name, plot_bbox, color_scale, max_all, min_all, \n",
    "                        min_all_diff, max_all_diff, vmin_manual, vmax_manual)\n",
    "\n",
    "        fig.suptitle(f'DISTRIBUTION OF {component_nom} (All times)',\n",
    "                    fontsize = 18, fontweight = 'bold', y = y)\n",
    "\n",
    "    elif distribution_type == 'individual':\n",
    "        \n",
    "        for time in plot_dates:\n",
    "\n",
    "            merge = merge_table.query('time == @time').to_xarray()\n",
    "            latitude = merge.sel(time = time).latitude\n",
    "            longitude = merge.sel(time = time).longitude\n",
    "            merge = merge.sel(time= time).assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize = (20, 5), subplot_kw = {'projection': projection})\n",
    "            fig.set_facecolor('w')\n",
    "            \n",
    "            im = create_maps(fig, axs, merge, range_type, sensor, model, sensor_type, model_type, \n",
    "                            projection, pad, units_name, plot_bbox, color_scale, max_all, min_all, \n",
    "                            min_all_diff, max_all_diff, vmin_manual, vmax_manual)\n",
    "            \n",
    "            if (sensor == 'iasi' and sensor_type == 'L3') or (sensor == 'gome' and sensor_type == 'L3'):\n",
    "                month = np.datetime64(time).astype('datetime64[M]')\n",
    "                fig.suptitle(f'DISTRIBUTION OF {component_nom} (Month: {month})',\n",
    "                            fontsize = 18, fontweight = 'bold', y = y)\n",
    "\n",
    "            else:\n",
    "                day = np.datetime64(time).astype('datetime64[D]')\n",
    "                fig.suptitle(f'DISTRIBUTION OF {component_nom} (Date: {day})',\n",
    "                            fontsize = 18, fontweight = 'bold', y = y)\n",
    "                             \n",
    "            plt.show()\n",
    "        \n",
    "    elif distribution_type == 'animated':\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize = (25, 10), subplot_kw = {'projection': projection})\n",
    "        fig.set_facecolor('w')\n",
    "\n",
    "        if (sensor == 'iasi' and sensor_type == 'L3') or (sensor == 'gome' and sensor_type == 'L3'):\n",
    "            month = np.datetime64(plot_dates[0]).astype('datetime64[M]')\n",
    "            fig_title = fig.text(0.5, 0.95, f'DISTRIBUTION OF {component_nom} (Month: {month})', \n",
    "                                 ha = 'center', fontsize = 22, fontweight = 'bold')\n",
    "\n",
    "        else:\n",
    "            day = np.datetime64(plot_dates[0]).astype('datetime64[D]')\n",
    "            fig_title = fig.text(0.5, 0.95, f'DISTRIBUTION OF {component_nom} (Date: {day})', \n",
    "                                 ha = 'center', fontsize = 22, fontweight = 'bold')\n",
    "\n",
    "        time = plot_dates[0]\n",
    "        merge = merge_table.query('time == @time').to_xarray()\n",
    "        latitude = merge.sel(time = time).latitude\n",
    "        longitude = merge.sel(time = time).longitude    \n",
    "        merge = merge.sel(time= time).assign_coords(latitude = latitude, longitude = longitude)    \n",
    "        im = create_maps(fig, axs, merge, range_type, sensor, model, sensor_type, model_type,\n",
    "                         projection, pad, units_name, plot_bbox, color_scale, \n",
    "                         max_all, min_all, min_all_diff, max_all_diff, vmin_manual, vmax_manual)\n",
    "\n",
    "        def animate(i):\n",
    "\n",
    "            time = plot_dates[i]\n",
    "            merge = merge_table.query('time == @time').to_xarray()\n",
    "            latitude = merge.sel(time = time).latitude\n",
    "            longitude = merge.sel(time = time).longitude\n",
    "            merge = merge.sel(time= time).assign_coords(latitude = latitude, longitude = longitude)\n",
    "            im = create_maps(fig, axs, merge, range_type, sensor, model, sensor_type, model_type, \n",
    "                             projection, pad, units_name, plot_bbox, color_scale, \n",
    "                             max_all, min_all, min_all_diff, max_all_diff, vmin_manual, vmax_manual)     \n",
    "\n",
    "            if (sensor == 'iasi' and sensor_type == 'L3') or (sensor == 'gome' and sensor_type == 'L3'):\n",
    "                month = np.datetime64(plot_dates[i]).astype('datetime64[M]')\n",
    "                fig_title.set_text(f'DISTRIBUTION OF {component_nom} (Month: {month})')\n",
    "\n",
    "            else:\n",
    "                day = np.datetime64(plot_dates[i]).astype('datetime64[D]')\n",
    "                fig_title.set_text(f'DISTRIBUTION OF {component_nom} (Date: {day})')\n",
    "\n",
    "            return im\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, animate, frames = len(plot_dates), blit = True, interval = 1000)\n",
    "\n",
    "        for j in range(0, 3):\n",
    "           \n",
    "            cbr = fig.colorbar(im[j], ax = axs[j], extend = 'both', orientation = 'horizontal', fraction = 0.05, pad = 0.15)\n",
    "            cbr.set_label(units_name, fontsize = 18) \n",
    "            cbr.ax.tick_params(labelsize = 16)\n",
    "            cbr.ax.xaxis.get_offset_text().set_fontsize(16)\n",
    "\n",
    "        display(HTML(anim.to_jshtml()))\n",
    "        anim.save('animation.mp4')\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        print('The distribution type (distribution_type) must be defined as aggregated, individual or animated.')\n",
    "        raise KeyboardInterrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_original_vs_calculated(model, component_nom, units, merge_table, \n",
    "                                           model_total_ds, plot_dates, plot_bbox, pad, y, \n",
    "                                           model_type, range_type, projection, color_scale,\n",
    "                                           vmin_manual = None, vmax_manual = None):\n",
    "\n",
    "    \"\"\" Plot model total columns from the original dataset and the calculated one \n",
    "        in the study area for the selected dates\n",
    "\n",
    "        Args:\n",
    "            model (str): Name of the model\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            units (str): Component units\n",
    "            merge_table (dataframe): Merge result\n",
    "            model_total_ds (xarray): CAMS total columns dataset in xarray format\n",
    "            plot_dates (arr): Plot dates\n",
    "            plot_bbox (arr): Plot extent\n",
    "            pad (float): Padding for the subtitles\n",
    "            y (float): y-position of main title\n",
    "            model_type (str): Model type:\n",
    "            -  'Forecast'\n",
    "            -  'Reanalysis'\n",
    "            range_type (str): Range type for colorbar:\n",
    "            -  'original': Show original values in range\n",
    "            -  'equal': Show same scale in range\n",
    "            projection: Geographical projection\n",
    "            color_scale (str): Name of color scale (e.g. coolwarm)\n",
    "            vmin_manual (float): Input vmin by user\n",
    "            vmax_manual (float): Input vmax by user\n",
    "    \"\"\"\n",
    "\n",
    "    units_name = component_nom + ' (' + units + ')'\n",
    "\n",
    "    # Get min and max before splitting the data into timesteps\n",
    "    min_model = np.nanmin(merge_table['model_column'])\n",
    "    max_model = np.nanmax(merge_table['model_column'])\n",
    "    min_sensor = np.nanmin(model_total_ds.component)\n",
    "    max_sensor = np.nanmax(model_total_ds.component)\n",
    "    max_all = max(max_sensor, max_model)\n",
    "    min_all = min(min_sensor, min_model)\n",
    "\n",
    "    for time in plot_dates:\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (20, 5), subplot_kw = {'projection': projection})\n",
    "        fig.set_facecolor('w')\n",
    "        \n",
    "        merge = merge_table.query('time == @time').to_xarray()\n",
    "        latitude = merge.sel(time = time).latitude\n",
    "        longitude = merge.sel(time = time).longitude\n",
    "        merge = merge.sel(time= time).assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "        step = 2\n",
    "\n",
    "        # First plot - CAMS calculated total columns\n",
    "        array = merge.model_column\n",
    "        vmin, vmax = colorbar_range(range_type, merge, array, max_all, min_all, units, vmin_manual, vmax_manual)\n",
    "        long_name = 'CALCULATED TOTAL COLUMNS ' + model.upper() + ' (' + model_type + ')'\n",
    "        visualize_pcolormesh(\n",
    "                            fig = fig, axs = axs[0],\n",
    "                            data_array = array.fillna(-999),\n",
    "                            longitude = array.longitude,\n",
    "                            latitude = array.latitude,\n",
    "                            projection = projection,\n",
    "                            color_scale = color_scale,\n",
    "                            pad = pad,\n",
    "                            long_name = long_name,\n",
    "                            units_name = units_name,\n",
    "                            vmin = vmin, \n",
    "                            vmax = vmax, \n",
    "                            lonmin = plot_bbox[0][0],\n",
    "                            lonmax = plot_bbox[1][0],\n",
    "                            latmin = plot_bbox[0][1],\n",
    "                            latmax = plot_bbox[1][1]\n",
    "                            )\n",
    "\n",
    "        # Second plot - CAMS original total columns\n",
    "        array = model_total_ds.component.isel(step = step).sel(time = time)\n",
    "        vmin, vmax = colorbar_range(range_type, merge, array, max_all, min_all, units, vmin_manual, vmax_manual)\n",
    "        long_name = 'ORIGINAL TOTAL COLUMNS ' + model.upper() + ' (' + model_type + ')'\n",
    "        visualize_pcolormesh(\n",
    "                            fig = fig, axs = axs[1],\n",
    "                            data_array = array.fillna(-999),\n",
    "                            longitude = array.longitude,\n",
    "                            latitude = array.latitude,\n",
    "                            projection = projection,\n",
    "                            color_scale = color_scale,\n",
    "                            pad = pad,\n",
    "                            long_name = long_name,\n",
    "                            units_name = units_name,\n",
    "                            vmin = vmin,\n",
    "                            vmax = vmax, \n",
    "                            lonmin = plot_bbox[0][0],\n",
    "                            lonmax = plot_bbox[1][0],\n",
    "                            latmin = plot_bbox[0][1],\n",
    "                            latmax = plot_bbox[1][1]\n",
    "                            )\n",
    "\n",
    "        day = np.datetime64(time).astype('datetime64[D]')\n",
    "        fig.suptitle(f'DISTRIBUTION OF {component_nom} (Date: {day})',\n",
    "                    fontsize = 18, fontweight = 'bold', y = y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_api():\n",
    "\n",
    "    \"\"\" Get Google API key for reverse geocoding (get country given the coordinates)\n",
    "        \n",
    "        Returns:\n",
    "            environ_keys[1]: Google API key\n",
    "    \"\"\"\n",
    "\n",
    "    # Open txt file with three lines:\n",
    "    # GOOGLE API KEY (first line), GOOGLE CLIENT ID (second line) and GOOGLE CLIENT SECRET (third line)\n",
    "    keys_file = open('data/keys.txt', 'r')\n",
    "    keys = keys_file.readlines()\n",
    "    environ_keys = [key.rstrip() for key in keys]\n",
    "\n",
    "    # Set environment variables in your system\n",
    "    os.environ['GOOGLE_API_KEY'] = environ_keys[1]\n",
    "    os.environ['GOOGLE_CLIENT'] = environ_keys[2]\n",
    "    os.environ['GOOGLE_CLIENT_SECRET'] = environ_keys[3]\n",
    "\n",
    "    return environ_keys[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(day):\n",
    "\n",
    "    \"\"\" Get season given the day\n",
    "\n",
    "        Args:\n",
    "            day (datetime): Date\n",
    "        \n",
    "        Returns:\n",
    "            season (str): Season of the year\n",
    "    \"\"\"\n",
    "\n",
    "    Y = 2000\n",
    "\n",
    "    seasons = [('Winter', (dt.date(Y,  1,  1),  dt.date(Y,  3, 20))),\n",
    "               ('Spring', (dt.date(Y,  3, 21),  dt.date(Y,  6, 20))),\n",
    "               ('Summer', (dt.date(Y,  6, 21),  dt.date(Y,  9, 22))),\n",
    "               ('Autumn', (dt.date(Y,  9, 23),  dt.date(Y, 12, 20))),\n",
    "               ('Winter', (dt.date(Y, 12, 21),  dt.date(Y, 12, 31)))]\n",
    "            \n",
    "    day = day.replace(year = Y)\n",
    "\n",
    "    season = next(season for season, (start, end) in seasons\n",
    "             if start <= day <= end)\n",
    "             \n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, Y, component_nom):\n",
    "\n",
    "    \"\"\" Fit a linear equation to scatter plot between X and Y and print results\n",
    "\n",
    "        Args:\n",
    "            X (array): Input sensor component values\n",
    "            Y (array): Input model component values\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "        \n",
    "        Returns:\n",
    "            fit_X (array): X in linear equation fit_Y = A * fit_X + B\n",
    "            fit_Y (array): Y in linear equation fit_Y = A * fit_X + B\n",
    "            score (float): Coefficient of determination\n",
    "            coefficient (float): A in linear equation fit_Y = A * fit_X + B\n",
    "            intercept (float): B in linear equation fit_Y = A * fit_X + B\n",
    "    \"\"\"\n",
    "\n",
    "    score = 'Unknown'\n",
    "    coefficient = 'Unknown'\n",
    "    intercept = 'Unknown'\n",
    "\n",
    "    reg = LinearRegression().fit(X, Y)\n",
    "    fit_X = np.linspace(np.nanmin(X), np.nanmax(X), 10)\n",
    "    fit_Y = fit_X * float(reg.coef_) + reg.intercept_\n",
    "    \n",
    "    score = reg.score(X, Y)\n",
    "    coefficient = reg.coef_[0][0]\n",
    "    intercept = reg.intercept_[0]\n",
    "\n",
    "    print(f'Fit equation: {component_nom}_model = {component_nom}_sensor * {float(reg.coef_):.2f} + ({float(reg.intercept_):.2E})')\n",
    "    print(f'Coefficient of determination (R2): {reg.score(X, Y):.2f}')\n",
    "\n",
    "    return fit_X, fit_Y, score, coefficient, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max):\n",
    "\n",
    "    \"\"\" Set common settings for scatter plots\n",
    "\n",
    "        Args:\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            plt (plot): Scatterplot\n",
    "            units (str): Component units\n",
    "            lim_min (float): Minimum value of component in scale\n",
    "            lim_max (float): Maximum value of component in scale\n",
    "    \"\"\"\n",
    "\n",
    "    # Scatter plot\n",
    "    axs[0].set_xlabel(f'Sensor {component_nom} ({units})', fontsize = 16)\n",
    "    axs[0].set_ylabel(f'Model {component_nom} ({units})', fontsize = 16)\n",
    "    axs[0].tick_params(labelsize = 14)\n",
    "    axs[0].set_xlim([lim_min, lim_max])\n",
    "    axs[0].set_ylim([lim_min, lim_max])\n",
    "\n",
    "    # Histograms\n",
    "    axs[1].set_xlabel(f'Sensor {component_nom} ({units})', fontsize = 16)\n",
    "    axs[2].set_xlabel(f'Model {component_nom} ({units})', fontsize = 16)\n",
    "    for i in range(1, 3):\n",
    "        axs[i].set_ylabel(f'Count', fontsize = 16)\n",
    "        axs[i].tick_params(labelsize = 14)\n",
    "        axs[i].set_xlim([lim_min, lim_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(merge_table, component_nom, units, sensor, plot_dates, y, extent_definition, \n",
    "                 show_seasons, scatter_plot_type, lim_min = None, lim_max = None, *args):\n",
    "\n",
    "    \"\"\" Scatter plot between the model and sensor datasets in the study area for the selected dates (bbox or countries)\n",
    "\n",
    "        Args:\n",
    "            merge_table (dataframe): Merge result\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            units (str): Component units\n",
    "            sensor (str): Name of the sensor\n",
    "            plot_dates (arr): Plot dates\n",
    "            plot_bbox (arr): Plot extent\n",
    "            y (float): y-position of main title\n",
    "            extent_definition (str):\n",
    "            * 'country': Scatter plots for countries list\n",
    "            * 'bbox': Scatter plots for bbox coordinates\n",
    "            scatter_plot_type (str):\n",
    "            * 'aggregated': Aggregate plots by time, country or season\n",
    "            * 'individual': Individual plots per time, country or season\n",
    "            *args: plot_countries, plot_bbox, lim_min, lim_max\n",
    "    \"\"\"\n",
    "\n",
    "    sns.color_palette('colorblind', 10)\n",
    "\n",
    "    if lim_min == None:\n",
    "        lim_min = min(np.nanmin(merge_table['sensor_column']), np.nanmin(merge_table['model_column']))\n",
    "    if lim_max == None:\n",
    "        lim_max = max(np.nanmax(merge_table['sensor_column']), np.nanmax(merge_table['model_column']))\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    merge = merge_table\n",
    "    merge = merge.query('longitude >= @plot_bbox[0][0] and longitude <= @plot_bbox[1][0] and latitude >= @plot_bbox[0][1] and latitude <= @plot_bbox[1][1]')\n",
    "\n",
    "    if show_seasons == False:\n",
    "\n",
    "        if extent_definition == 'bbox':\n",
    "\n",
    "            if scatter_plot_type == 'aggregated':\n",
    "\n",
    "                # Prepare df\n",
    "                merge = merge.reset_index()\n",
    "                merge = merge[merge['time'].isin(plot_dates)]\n",
    "\n",
    "                if not merge.empty:\n",
    "                    \n",
    "                    fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "                    fig.set_facecolor('w')\n",
    "                    \n",
    "                    # Linear regression\n",
    "                    X = merge['sensor_column'].values.reshape(-1, 1) \n",
    "                    Y = merge['model_column'].values.reshape(-1, 1) \n",
    "                    fit_X, fit_Y, score, coefficient, intercept = linear_regression(X, Y, component_nom)\n",
    "                    axs[0].plot(fit_X, fit_Y, color = 'black', label = 'Linear regression')\n",
    "                    \n",
    "                    # Scatter plot and histograms\n",
    "                    sp = sns.scatterplot(data = merge, x = 'sensor_column', y = 'model_column', \n",
    "                                         hue = 'time', ax = axs[0])\n",
    "                    sns.histplot(data = merge, x = 'sensor_column', kde = True,  ax = axs[1])\n",
    "                    sns.histplot(data = merge, x = 'model_column', kde = True,  ax = axs[2])\n",
    "\n",
    "                    scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max)\n",
    "                    fig.suptitle(f'{component_nom} (All times)', fontsize = 18, fontweight = 'bold', y = y)\n",
    "\n",
    "                    # Scatter plot legend\n",
    "                    if sensor_type == 'L3':\n",
    "\n",
    "                        leg = sp.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.2),\n",
    "                                        fancybox = True, ncol = 3, fontsize = 14)\n",
    "                        leg.set_title('Fit and months', prop = {'size': 14})\n",
    "\n",
    "                    else:\n",
    "                                        \n",
    "                        leg = sp.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.2),\n",
    "                                        fancybox = True, ncol = 3, fontsize = 14)\n",
    "                        leg.set_title('Fit and dates', prop = {'size': 14})\n",
    "\n",
    "                    # Update summary\n",
    "                    summary.append({'Period': plot_dates, 'Location': plot_bbox, \n",
    "                                    'Score': score, 'Coefficient': coefficient, \n",
    "                                    'Intercept': intercept})\n",
    "\n",
    "            elif scatter_plot_type == 'individual':\n",
    "                \n",
    "                for time in plot_dates:\n",
    "                    \n",
    "                    # Prepare df\n",
    "                    merge_time = merge.query('time == @time')\n",
    "                    \n",
    "                    if not merge_time.empty:\n",
    "                        \n",
    "                        fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "                        fig.set_facecolor('w')\n",
    "\n",
    "                        # Scatter plot and histograms\n",
    "                        sp = sns.scatterplot(data = merge_time, x = 'sensor_column', y = 'model_column', ax = axs[0])\n",
    "                        sns.histplot(data = merge_time, x = 'sensor_column', kde = True,  ax = axs[1])\n",
    "                        sns.histplot(data = merge_time, x = 'model_column', kde = True,  ax = axs[2])\n",
    "\n",
    "                        scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max)\n",
    "                        \n",
    "                        if sensor == 'tropomi' or (sensor == 'gome' and sensor_type == 'L2'):\n",
    "                            day = np.datetime64(time).astype('datetime64[D]')\n",
    "                            fig.suptitle(f'{component_nom} (Date: {day})', \n",
    "                                         fontsize = 18, fontweight = 'bold', y = y)\n",
    "                            \n",
    "                        else:\n",
    "                            month = np.datetime64(time).astype('datetime64[M]')\n",
    "                            fig.suptitle(f'{component_nom} (Month: {month})', \n",
    "                                         fontsize = 18, fontweight = 'bold', y = y)\n",
    "\n",
    "                        # Linear regression\n",
    "                        X = merge_time['sensor_column'].values.reshape(-1, 1) \n",
    "                        Y = merge_time['model_column'].values.reshape(-1, 1) \n",
    "                        fit_X, fit_Y, score, coefficient, intercept = linear_regression(X, Y, component_nom)\n",
    "                        axs[0].plot(fit_X, fit_Y, color = 'black', label = 'Linear regression')\n",
    "                        plt.show()\n",
    "\n",
    "                        # Update summary\n",
    "                        summary.append({'Period': time, 'Location': plot_bbox, \n",
    "                                        'Score': score, 'Coefficient': coefficient, \n",
    "                                        'Intercept': intercept})\n",
    "\n",
    "        elif extent_definition == 'country':\n",
    "            \n",
    "            # Prepare df\n",
    "            merge = merge.reset_index()\n",
    "            merge = merge[merge['time'].isin(plot_dates)]\n",
    "\n",
    "            # Read Google API key for reverse geocoding (get country by coordinates)\n",
    "            google_api_key = get_google_api()\n",
    "\n",
    "            # Reverse geocoding\n",
    "            merge['Country'] = merge.apply(lambda row: geocoder.google([row['latitude'], row['longitude']], \n",
    "                                            method='reverse', key = google_api_key).country_long, axis = 1)\n",
    "\n",
    "            # Find data for the countries in search list\n",
    "            merge = merge[merge['Country'].isin(plot_countries)]\n",
    "            available_countries = np.unique(merge['Country'])\n",
    "\n",
    "            if scatter_plot_type == 'aggregated':\n",
    "\n",
    "                if not merge.empty:\n",
    "                    \n",
    "                    fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "                    fig.set_facecolor('w')\n",
    "\n",
    "                    # Linear regression\n",
    "                    X = merge['sensor_column'].values.reshape(-1, 1) \n",
    "                    Y = merge['model_column'].values.reshape(-1, 1) \n",
    "                    fit_X, fit_Y, score, coefficient, intercept = linear_regression(X, Y, component_nom)\n",
    "                    axs[0].plot(fit_X, fit_Y, color = 'black', label = 'Linear regression')\n",
    "\n",
    "                    # Scatter plot and histograms\n",
    "                    sp = sns.scatterplot(data = merge, x = 'sensor_column', y = 'model_column', \n",
    "                                         hue = 'Country', ax = axs[0])\n",
    "                    sns.histplot(data = merge, x = 'sensor_column', kde = True,  ax = axs[1])\n",
    "                    sns.histplot(data = merge, x = 'model_column', kde = True,  ax = axs[2])\n",
    "\n",
    "                    scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max)\n",
    "                    fig.suptitle(f'{component_nom} (All countries)', fontsize = 18, fontweight = 'bold', y = y)\n",
    "                    leg = sp.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.2),\n",
    "                                    fancybox = True, ncol = 3, fontsize = 14)\n",
    "                    leg.set_title('Fit and countries', prop = {'size': 14})\n",
    "\n",
    "                    # Update summary\n",
    "                    summary.append({'Period': plot_dates, 'Location': available_countries, \n",
    "                                    'Score': score, 'Coefficient': coefficient, \n",
    "                                    'Intercept': intercept})\n",
    "\n",
    "            elif scatter_plot_type == 'individual':\n",
    "\n",
    "                for plot_country in plot_countries:\n",
    "\n",
    "                    merge_country = merge[merge['Country'] == plot_country]\n",
    "\n",
    "                    if not merge_country.empty:\n",
    "                        \n",
    "                        fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "                        fig.set_facecolor('w')\n",
    "\n",
    "                        # Linear regression\n",
    "                        X = merge_country['sensor_column'].values.reshape(-1, 1) \n",
    "                        Y = merge_country['model_column'].values.reshape(-1, 1) \n",
    "                        fit_X, fit_Y, score, coefficient, intercept = linear_regression(X, Y, component_nom)\n",
    "                        axs[0].plot(fit_X, fit_Y, color = 'black')\n",
    "\n",
    "                        # Update summary\n",
    "                        summary.append({'Period': plot_dates, 'Location': plot_country, \n",
    "                                        'Score': score, 'Coefficient': coefficient, \n",
    "                                        'Intercept': intercept})\n",
    "\n",
    "                        # Scatter plot and histograms\n",
    "                        sp = sns.scatterplot(data = merge_country, x = 'sensor_column', y = 'model_column', ax = axs[0])\n",
    "                        sns.histplot(data = merge_country, x = 'sensor_column', kde = True,  ax = axs[1])\n",
    "                        sns.histplot(data = merge_country, x = 'model_column', kde = True,  ax = axs[2])\n",
    "\n",
    "                        scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max)\n",
    "                        fig.suptitle(f'{component_nom} ({plot_country})', fontsize = 18, fontweight = 'bold', y = y)\n",
    "                        plt.show()\n",
    "\n",
    "            else:\n",
    "                print('ERROR: scatter_plot_type is wrongly defined. The options are ''aggregated'' and ''individual''.')\n",
    "                raise KeyboardInterrupt()\n",
    "\n",
    "        else:\n",
    "            print('ERROR: extent_definition is wrongly defined. The options are ''bbox'' and ''country''.')\n",
    "            raise KeyboardInterrupt()\n",
    "                \n",
    "    elif show_seasons == True:\n",
    "        \n",
    "        if show_seasons == True and extent_definition == 'country':\n",
    "            print('ERROR: Set up show_seasons to False in order to show the scatter plots by countries.')\n",
    "            raise KeyboardInterrupt()\n",
    "\n",
    "        plot_seasons = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "\n",
    "        # Prepare df\n",
    "        merge = merge.reset_index()\n",
    "        merge = merge[merge['time'].isin(plot_dates)]\n",
    "\n",
    "        # Find data for the seasons in list\n",
    "        merge['Season'] = merge.apply(lambda row: get_season(row['time']), axis = 1)\n",
    "        available_seasons = np.unique(merge['Season'])\n",
    "\n",
    "        if scatter_plot_type == 'aggregated':\n",
    "\n",
    "            if not merge.empty:\n",
    "                \n",
    "                fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "                fig.set_facecolor('w')\n",
    "\n",
    "                # Linear regression\n",
    "                X = merge['sensor_column'].values.reshape(-1, 1) \n",
    "                Y = merge['model_column'].values.reshape(-1, 1) \n",
    "                fit_X, fit_Y, score, coefficient, intercept = linear_regression(X, Y, component_nom)\n",
    "                axs[0].plot(fit_X, fit_Y, color = 'black', label = 'Linear regression')\n",
    "\n",
    "                # Scatter plot and histograms\n",
    "                sp = sns.scatterplot(data = merge, x = 'sensor_column', y = 'model_column', \n",
    "                                     hue = 'Season', ax = axs[0])\n",
    "                sns.histplot(data = merge, x = 'sensor_column', kde = True, ax = axs[1])\n",
    "                sns.histplot(data = merge, x = 'model_column', kde = True, ax = axs[2])\n",
    "\n",
    "                scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max)\n",
    "                fig.suptitle(f'{component_nom} (All seasons)', fontsize = 18, fontweight = 'bold', y = y)\n",
    "                leg = sp.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.2),\n",
    "                                fancybox = True, ncol = 3, fontsize = 14)\n",
    "                leg.set_title('Fit and seasons', prop = {'size': 14})\n",
    "                plt.show()\n",
    "\n",
    "                # Update summary\n",
    "                summary.append({'Period': available_seasons, 'Location': plot_bbox, \n",
    "                                'Score': score, 'Coefficient': coefficient, \n",
    "                                'Intercept': intercept})\n",
    "\n",
    "        elif scatter_plot_type == 'individual':\n",
    "\n",
    "            for plot_season in plot_seasons:\n",
    "                \n",
    "                # Prepare df\n",
    "                merge_season = merge[merge['Season'] == plot_season]\n",
    "                \n",
    "                if not merge_season.empty:\n",
    "                    \n",
    "                    fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "                    fig.set_facecolor('w')\n",
    "\n",
    "                    # Linear regression\n",
    "                    X = merge_season['sensor_column'].values.reshape(-1, 1) \n",
    "                    Y = merge_season['model_column'].values.reshape(-1, 1) \n",
    "                    fit_X, fit_Y, score, coefficient, intercept = linear_regression(X, Y, component_nom)\n",
    "                    axs[0].plot(fit_X, fit_Y, color = 'black')\n",
    "\n",
    "                    # Update summary\n",
    "                    summary.append({'Period': plot_season, 'Location':  plot_bbox, \n",
    "                                    'Score': score, 'Coefficient': coefficient, \n",
    "                                    'Intercept': intercept})\n",
    "\n",
    "                    # Scatter plot and histograms\n",
    "                    sp = sns.scatterplot(data = merge_season, x = 'sensor_column', y = 'model_column', ax = axs[0])\n",
    "                    sns.histplot(data = merge_season, x = 'sensor_column', kde = True, ax = axs[1])\n",
    "                    sns.histplot(data = merge_season, x = 'model_column', kde = True, ax = axs[2])\n",
    "\n",
    "                    fig.suptitle(f'{component_nom} ({plot_season})', fontsize = 18, fontweight = 'bold', y = y)\n",
    "                    scatter_plot_general_settings(component_nom, axs, units, lim_min, lim_max)\n",
    "                    plt.show()\n",
    "\n",
    "        else:\n",
    "            print('ERROR: scatter_plot_type is wrongly defined. The options are ''aggregated'' and ''individual''.')\n",
    "            raise KeyboardInterrupt()\n",
    "\n",
    "    else:\n",
    "        print('ERROR: show_seasons is wrongly defined. The options are True and False.')\n",
    "        raise KeyboardInterrupt()\n",
    "    \n",
    "    summary = pd.DataFrame(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_coords(merge_table, coords_search_list, component_nom, sensor, model, plot_dates, units):\n",
    "\n",
    "    \"\"\" Get component data for the closest coordinates to the list of search coordinates and plot them along time\n",
    "\n",
    "        Args:\n",
    "            merge_table (dataframe): Merge result\n",
    "            coords_search_list (list): List of search coordinates\n",
    "            component_nom (str): Component chemical nomenclature\n",
    "            sensor (str): Name of the sensor\n",
    "            model (str): Name of the model\n",
    "            plot_dates (arr): Plot dates\n",
    "            units (str): Component units\n",
    "\n",
    "        Returns:\n",
    "            retrieval_table_all (dataframe): Dataframe with results from search\n",
    "    \"\"\"\n",
    "    \n",
    "    retrieval_table_all = pd.DataFrame()\n",
    "\n",
    "    coords_search = pairwise(coords_search_list)\n",
    "\n",
    "    for i in range(0, len(coords_search)):\n",
    "\n",
    "        for time in plot_dates:\n",
    "\n",
    "            # List of available points per time\n",
    "            retrieval_table = merge_table.query('time == @time').reset_index()\n",
    "            available_points = list([(x, y) for x, y in zip(retrieval_table['latitude'], retrieval_table['longitude'])])\n",
    "            \n",
    "            # Get closest pair to coordinates in search list\n",
    "            lat_found = closest_point(coords_search[i], available_points)[0]\n",
    "            lon_found = closest_point(coords_search[i], available_points)[1]\n",
    "            retrieval_table = merge_table.query('latitude == @lat_found and longitude == @lon_found and time == @time')\n",
    "\n",
    "            retrieval_table = retrieval_table.reset_index()\n",
    "            retrieval_table['lat_search'] = coords_search[i][0]\n",
    "            retrieval_table['lon_search'] = coords_search[i][1]\n",
    "        \n",
    "            # Append retrieval table to previous coordinates\n",
    "            retrieval_table_all = retrieval_table_all.append(retrieval_table)\n",
    "        \n",
    "        table_length = len(retrieval_table_all[(retrieval_table_all['lat_search'] == coords_search[i][0]) &\n",
    "                                               (retrieval_table_all['lon_search'] == coords_search[i][1])])\n",
    "\n",
    "        # Plot variations in time\n",
    "        if table_length > 1:\n",
    "        \n",
    "            fig, ax = plt.subplots(figsize = (30, 5))\n",
    "            fig.set_facecolor('w')\n",
    "\n",
    "            retrieval_table_time = retrieval_table_all[(retrieval_table_all['lat_search'] == coords_search[i][0]) & \n",
    "                                                       (retrieval_table_all['lon_search'] == coords_search[i][1])]\n",
    "            plt1 = ax.plot(retrieval_table_time['time'], retrieval_table_time['sensor_column'], color = 'red', label = sensor.upper())\n",
    "            plt2 = ax.plot(retrieval_table_time['time'], retrieval_table_time['model_column'], color = 'black', label = model.upper())\n",
    "\n",
    "            ax.legend(loc='center left', bbox_to_anchor = (1, 0.5), prop = {'size': 25})\n",
    "\n",
    "            if sensor == 'tropomi':\n",
    "                ax.set_xlabel('Estimated time', fontsize = 25)\n",
    "                \n",
    "            elif sensor == 'iasi' or sensor == 'gome':\n",
    "                ax.set_xlabel('Month', fontsize = 25)\n",
    "\n",
    "            ax.tick_params(labelsize = 22)\n",
    "            ax.set_ylabel(f'{component_nom} ({units})', fontsize = 25)\n",
    "            ax.set_title(f'{component_nom} total column near ({coords_search[i][0]}, {coords_search[i][1]})', \n",
    "                         fontsize = 25, fontweight = 'bold', y = 1.05)\n",
    "        \n",
    "    retrieval_table_all = retrieval_table_all.set_index(['lat_search', \n",
    "                                                         'lon_search', \n",
    "                                                         'latitude', \n",
    "                                                         'longitude', \n",
    "                                                         'time'])\n",
    "\n",
    "    return retrieval_table_all"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc2967d46b8688a8c6de8a18a3daae8ebe0b7dc5d18d27687b3fe01b2a6426f9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env-new': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
