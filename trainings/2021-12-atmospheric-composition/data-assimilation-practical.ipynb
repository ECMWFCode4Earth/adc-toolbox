{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/organizations-logos.png' align='center' width='100%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Assimilation Practical Â· Joint Training in Atmospheric Composition (2021)\n",
    "\n",
    "This practical exercise was created for the <a href =\"https://atmosphere.copernicus.eu/3rd-eumetsatesaecmwf-joint-training-atmospheric-composition\" target = \"_blank\">3rd EUMETSAT/ESA/ECMWF Joint Training in Atmospheric Composition (6-17 December, 2021)</a> to show the assimilation of NO<sub>2</sub> observations from the TROPOspheric Monitoring Instrument (TROPOMI) aboard Sentinel 5-P into the forecasts of the Copernicus Atmosphere Monitoring Service (CAMS). It is divided into the following sections:\n",
    "\n",
    "1. [Installation](#installation): A brief guide to know how to install the <a href = \"https://github.com/esowc/adc-toolbox/\" target = \"_blank\">Atmospheric Datasets Comparison (ADC) Toolbox</a>, which contains functions that facilitate the datasets retrieval, metadata merge and statistical analysis.\n",
    "\n",
    "2. [Datasets retrieval](#datasets_retrieval): The model and sensor datasets are downloaded and read as xarray objects before assimilating the real observations into the model dataset.\n",
    "\n",
    "3. [Units conversion](#units_conversion): The units of both datasets are converted to molecules/cm<sup>2</sup>.\n",
    "\n",
    "4. [Data assimilation](#data_assimilation): The model partial columns are interpolated into the TM5 grid and the averaging kernels are applied.\n",
    "\n",
    "5. [Comparison analysis](#comparison_analysis): Statistical methods are used to better understand the differences between both datasets and the effects of the data assimilation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='installation'></a>1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the repository and set up the virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participants should <a href = \"https://my.wekeo.eu/web/guest/user-registration\" target = \"_blank\">create an account in Wekeo</a> to use the JupyterHub and run this notebook. Once they <a href = \"https://jupyterhub-wekeo.apps.eumetsat.dpi.wekeo.eu\" target = \"_blank\">have access to this service</a>, they can open the terminal, or an empty Jupyter Notebook, and clone the ADC Toolbox repository with the command:\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/esowc/adc-toolbox\n",
    "```\n",
    "\n",
    "The virtual environment <em>environment.yml</em> was generated to simplify the installation process, so you just need to create this environment from the file and activate it to install the necessary libraries and packages. Since this process might take up some time, it is better if the users simulate it by:\n",
    "\n",
    "```bash\n",
    "$ conda create --name adc-toolbox\n",
    "$ conda activate adc-toolbox\n",
    "$ conda install -c conda-forge/label/cartopy_dev cartopy\n",
    "$ pip install -r requirements.txt\n",
    "$ python -m ipykernel install --user --name adc-toolbox\n",
    "```\n",
    "\n",
    "After running the previous commands, the page should be refreshed and the correct kernel (adc-toolbox) should be selected.\n",
    "\n",
    "To finalize the installation process, users need to create a text file under the training data folder, with the name <em>keys.txt</em>, and write down their personal CAMS API key in one line with the format <em>UID:Key</em>. This key can be obtained by <a href = \"https://ads.atmosphere.copernicus.eu/user/register?\">registering at the Atmosphere Data Store</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Related to the system\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "# Related to the data retrieval\n",
    "from sentinelsat.sentinel import SentinelAPI, geojson_to_wkt\n",
    "import cdsapi\n",
    "import cfgrib\n",
    "import geojson\n",
    "import urllib3\n",
    "\n",
    "# Related to the data analysis\n",
    "import math\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from itertools import product\n",
    "import scipy.interpolate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Related to the results\n",
    "from copy import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "import geocoder\n",
    "import seaborn as sns\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../functions/functions_general.ipynb\n",
    "%run ../../functions/functions_cams.ipynb\n",
    "%run ../../functions/functions_tropomi.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide pandas warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Hide API request warning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Increase animation limit\n",
    "matplotlib.rcParams['animation.embed_limit'] = 25000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='datasets_retrieval'></a>2. Datasets retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available datasets\n",
    "\n",
    "ADC Toolbox facilitates the data retrieval of all the datasets presented in Table 1, since the dates they became available to the public. As an exception, the retrieval of IASI L2 data is currently available only since May 14, 2019.\n",
    "\n",
    "<p align=\"center\"> Table 1. Temporal availability (start date - present) by data source.</p>\n",
    "\n",
    "| Dataset | Type | NO<sub>2</sub> | O<sub>3</sub> | CO | SO<sub>2</sub> | HCHO |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| CAMS  | <a href = \"https://ads.atmosphere.copernicus.eu/cdsapp#!/dataset/CAMS-global-atmospheric-composition-forecasts\" target = \"_blank\">Forecast</a> | 01.2015 | 01.2015 | 01.2015 | 01.2015 | 01.2015 | \n",
    "| CAMS  | <a href = \"https://ads.atmosphere.copernicus.eu/cdsapp#!/dataset/CAMS-global-ghg-reanalysis-egg4-monthly\">Reanalysis</a> | 01.2003 | 01.2003 | 01.2003 | 01.2003 | 01.2003 | \n",
    "| TROPOMI  | <a href = \"https://s5phub.copernicus.eu/dhus/\" target = \"_blank\">L2</a> | 07.2018 | 07.2018 | 07.2018 | 10.2018 | - | \n",
    "| IASI  | <a href = \"https://iasi.aeris-data.fr/\" target = \"_blank\">L2</a> | - | 01.2008 | 10.2007 | 10.2007 | - |\n",
    "| IASI  | <a href = \"https://iasi.aeris-data.fr/\" target = \"_blank\">L3</a> | - | 01.2008 | 10.2007 | 10.2007 | - |\n",
    "| GOME-2  | <a href = \"https://acsaf.org/offline_access.php\" target = \"_blank\">L2</a> | 01.2007 | 01.2007 | - | 01.2007 | 01.2007 | \n",
    "| GOME-2  | <a href = \"https://acsaf.org/offline_access.php\" target = \"_blank\">L3</a> | 02.2007 | 01.2007 | - | 01.2007 | 01.2007 | \n",
    "\n",
    "Sentinel 5-P was launched in 2017 and, as it can be read in the table, the L2 products are accessible since:\n",
    "\n",
    "* July 2018 for NO<sub>2</sub>, O<sub>3</sub> and CO concentrations.\n",
    "* October 2018 for SO<sub>2</sub> concentrations.\n",
    "\n",
    "In order to automatically download any model or sensor dataset, users only need to define:\n",
    "\n",
    "* Name of the atmospheric component.\n",
    "* Short (<em>cams</em>) and full name of the model (<em>cams-global-atmospheric-composition-forecasts</em> or <em>cams-global-reanalysis-eac4-monthly</em>).\n",
    "* Short name of the sensor (<em>tropomi</em>, <em>iasi</em> or <em>gome</em>).\n",
    "* Start and end date: a range will be calculated from both dates.\n",
    "* Bounding box by coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define component\n",
    "component_nom = 'NO2'\n",
    "\n",
    "# Define model\n",
    "model = 'cams'\n",
    "model_full_name = 'cams-global-atmospheric-composition-forecasts'\n",
    "\n",
    "# Define sensor\n",
    "sensor = 'tropomi'\n",
    "sensor_type = 'L2'\n",
    "apply_kernels = True\n",
    "\n",
    "# Define search period\n",
    "start_date = '2021-11-24'\n",
    "end_date = '2021-11-24'\n",
    "\n",
    "# Define extent\n",
    "lon_min = 16\n",
    "lon_max = 18.9\n",
    "lat_min = 48.5\n",
    "lat_max = 51.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison checker and folder generation\n",
    "\n",
    "The toolbox will check if the merge and comparison between the specified model and sensor is possible, given the name of the species. If it is, the molecular weight and some metadata will be obtained. Afterwards, it will create the folders where the datasets will be stored.\n",
    "\n",
    "This notebook can only be used to carry out the data assimilation process, please refer to the main code in case you wish to compare the CAMS model against the observations from IASI or GOME-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if comparison is possible\n",
    "comparison_check(sensor, model, component_nom, model_full_name, sensor_type)\n",
    "\n",
    "# Get component full name and molecular weight\n",
    "component, component_mol_weight, product_type, sensor_column = components_table(sensor, component_nom, sensor_type)\n",
    "\n",
    "# Folders generation\n",
    "generate_folders(model, sensor, component_nom, sensor_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search period and bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search period and bounding box are derived from the details that were provided in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate array with search dates\n",
    "dates = search_period(start_date, end_date, sensor, sensor_type)\n",
    "\n",
    "# Create bbox\n",
    "bbox = search_bbox(lon_min, lat_min, lon_max, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read the model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model dataset will be downloaded as a GRIB file and read as a xarray object. In this step, the users can decide if they want to retrieve total (<em>model_level = 'Single'</em>) or partial columns (<em>model_level = 'Multiple'</em>). In the data assimilation process, we need to obtain the partial columns and, particularly in this case, at 137 vertical levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_product_name, model_type = CAMS_download(dates, start_date, end_date, component, \n",
    "                                               component_nom, model_full_name, model_level = 'Multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ds, _ = CAMS_read(model_product_name, component, component_nom, dates)\n",
    "model_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensor dataset will be downloaded as a NetCDF file and read as a xarray object, along with its details in the attached datasets. More information about the product can be found in the <a href = \"http://www.tropomi.eu/sites/default/files/files/S5P-KNMI-L2-0021-MA-Product_User_Manual_for_the_Sentinel_5_precursor_Nitrogen_dioxide-0.8.1_20151207_signed.pdf\" target = \"_blank\">TROPOMI NO<sub>2</sub> product manual</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sensor_download(sensor, sensor_type, component_nom, dates, bbox, product_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds, support_input_ds, support_details_ds = sensor_read(sensor, sensor_type, sensor_column, \n",
    "                                                              component_nom, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within <em>support_input_ds</em> we can find the surface pressure data that we need to compute the pressure at each level, while <em>support_details_ds</em> contains the processing quality flags and air mass factors to calculate the column kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_input_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_details_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='units-conversion'></a>3. Units conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model data units (from kg/kg to molecules/cm<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve auxiliary data\n",
    "\n",
    "The details of the 137 vertical levels in the L137 model are defined by ECMWF and are necessary to evaluate the levels pressure. From the information given, we will particularly need the coefficients <em>a</em> and <em>b</em>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_levels_df = CAMS_137_levels()\n",
    "model_levels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the columns above each half level (kg/kg to kg/m<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the original units (kg/kg) into molecules/cm<sup>2</sup>, we need to calculate the level pressures and the NO<sub>2</sub> columns above each CAMS half level assuming that they are 0 at the top of the atmosphere, converting the units to kg/m<sup>2</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate level pressures from the surface pressures\n",
    "model_ds = CAMS_pressure(model_ds, model_levels_df, start_date, end_date, component_nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The columns above each model half level will be calculated.')\n",
    "\n",
    "# Initialize new array\n",
    "model_ds_all = []\n",
    "\n",
    "for time in model_ds.time:\n",
    "\n",
    "    PC_hybrid = []\n",
    "    \n",
    "    model_ds_time_old = model_ds.sel(time = time)\n",
    "\n",
    "    # Initialize partial columns at the top of the atmosphere as 0\n",
    "    PC_hybrid_0 = model_ds_time_old.sel(hybrid = 1)\n",
    "    PC_hybrid_0['component'] = PC_hybrid_0['component'].where(PC_hybrid_0['component'] <= 0, 0, drop = False)\n",
    "    PC_hybrid_0 = PC_hybrid_0.expand_dims(dim = ['hybrid'])\n",
    "    PC_hybrid.append(PC_hybrid_0)\n",
    "    model_ds_time_new = PC_hybrid_0\n",
    "\n",
    "    for hybrid in range(1, 136):\n",
    "\n",
    "        # Calculate partial columns above each model level\n",
    "        PC_last = model_ds_time_new.component.sel(hybrid = hybrid)\n",
    "        PC_current = model_ds_time_old.component.sel(hybrid = hybrid + 1)\n",
    "        pressure_last = model_ds_time_old.pressure.sel(hybrid = hybrid)\n",
    "        pressure_current = model_ds_time_old.pressure.sel(hybrid = hybrid + 1)\n",
    "        pressure_diff = pressure_current - pressure_last\n",
    "\n",
    "        # Units: (kg/kg * kg/m*s2) * s2/m -> kg/m2\n",
    "        PC_above = model_ds_time_old.sel(hybrid = hybrid + 1)\n",
    "        PC_above['component'] = PC_last + PC_current * pressure_diff * (1/9.81)\n",
    "        PC_hybrid.append(PC_above)\n",
    "        model_ds_time_new = xr.concat(PC_hybrid, pd.Index(range(1, hybrid + 2), name = 'hybrid'))\n",
    "\n",
    "    model_ds_all.append(model_ds_time_new)\n",
    "\n",
    "model_ds = xr.concat(model_ds_all, dim = 'time')\n",
    "\n",
    "# Assign new units to array\n",
    "units = 'kg m**-2'\n",
    "model_ds['component'] = model_ds.component.assign_attrs({'units': units})\n",
    "print('The model component units have been converted from kg kg**-1 to kg m**-2.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert units with Avogadro's number (kg/m<sup>2</sup> to molecules/cm<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we convert the data units from kg/m<sup>2</sup> to molecules/cm<sup>2</sup> simply by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Avogadro's number\n",
    "NA = 6.022*10**23\n",
    "model_ds['component'] = (model_ds['component'] * NA * 1000) / (10000 * component_mol_weight)\n",
    "\n",
    "# Assign new units to array\n",
    "units = 'molec cm-2'\n",
    "model_ds['component'] = model_ds.component.assign_attrs({'units': units})\n",
    "print('The model component units have been converted from kg m**-2 to molec cm-2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TROPOMI data units (From mol/m<sup>2</sup> to molecules/cm<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds['sensor_column'] = sensor_ds['sensor_column'] * 6.02214*10**19\n",
    "sensor_ds['sensor_column'] = sensor_ds['sensor_column'].assign_attrs({'units': 'molec cm-2'})\n",
    "\n",
    "print('The sensor component units have been converted from mol cm-2 to molec cm-2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='data_assimilation'></a>4. Data assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TM5 level pressures, column kernels and apriori profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_table = pd.DataFrame()\n",
    "\n",
    "print('APPLICATION OF AVERAGING KERNELS')\n",
    "print('For the application of the averaging kernels, it is necessary to calculate or retrieve:')\n",
    "print('1. Level pressures')\n",
    "print('2. Column kernels')\n",
    "print('The apriori profiles should be retrieved too, but they are not necessary.')\n",
    "\n",
    "print('DATA AVAILABILITY')\n",
    "sensor_ds = TROPOMI_pressure(sensor_ds, component_nom, support_input_ds, support_details_ds)\n",
    "sensor_ds = TROPOMI_column_kernel(sensor_ds, component_nom, support_details_ds)\n",
    "sensor_ds = TROPOMI_apriori_profile(sensor_ds, component, support_details_ds)\n",
    "\n",
    "sensor_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting the datasets will speed up the data assimilation process. CAMS dataset was already reduced to the size of the previously defined bounding box. To subset TROPOMI's dataset, we create a lookup table with the equivalent geospatial coordinates to each pair of scanline and ground pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first date to see all the steps\n",
    "time = sensor_ds.time.values[0]\n",
    "\n",
    "# Reduce data to only one timestamp\n",
    "model_ds_time = model_ds.sel(time = time)\n",
    "sensor_ds_time = sensor_ds.sel(time = time)\n",
    "\n",
    "# Subset\n",
    "sensor_ds_time = TROPOMI_subset(sensor_ds_time, bbox, time, sensor, component_nom)\n",
    "\n",
    "# Read new coordinates (after subset)\n",
    "sensor_coords_df = TROPOMI_subset_lookup_table(sensor_ds_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_coords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform into dataframe and change multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data array into dataframe\n",
    "match_df = sensor_ds_time.to_dataframe()\n",
    "\n",
    "# Pass NaNs to data with qa_value under 0.5 (these values will be shown as transparent)\n",
    "match_df.loc[match_df['qa_value'] <= 0.5, ['sensor_column', 'column_kernel']] = float('NaN')\n",
    "\n",
    "# Drop levels\n",
    "if component_nom == 'CO' or component_nom == 'SO2':\n",
    "    \n",
    "    match_df.index.names = ['corner', 'ground_pixel', 'layer', 'scanline']\n",
    "\n",
    "elif component_nom == 'O3':\n",
    "\n",
    "    match_df.index.names = ['corner', 'ground_pixel', 'layer', 'level', 'scanline']\n",
    "\n",
    "# Select multiindex elements\n",
    "match_df = match_df.groupby(by = ['layer', 'scanline', 'ground_pixel', 'time', 'delta_time']).mean()\n",
    "match_df = match_df.reset_index(level = ['delta_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include CAMS pressure levels into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index that includes CAMS pressure levels for all the locations in TROPOMI\n",
    "new_array = np.concatenate([np.arange(1, 137) * 1000, sensor_ds_time.layer.values])\n",
    "new_index = pd.MultiIndex.from_product([match_df.index.levels[0], \n",
    "                                        match_df.index.levels[1],\n",
    "                                        match_df.index.levels[2],\n",
    "                                        new_array],\n",
    "                                        names = ['scanline', 'ground_pixel', 'time', 'hybrid'])\n",
    "\n",
    "# Append original and new indexes and reindex dataframe\n",
    "match_df = match_df[~match_df.index.duplicated()]\n",
    "match_df = match_df.reindex(match_df.index.append(new_index))\n",
    "\n",
    "# Sort and reset index\n",
    "match_df = match_df.sort_index()\n",
    "match_df = match_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve CAMS partial columns at TM5 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latitudes in CAMS rows with scanlines and ground pixels\n",
    "match_df['latitude'] = match_df.apply(lambda row: float(sensor_coords_df[\n",
    "                                                       (sensor_coords_df['scanline'] == row['scanline']) & \n",
    "                                                       (sensor_coords_df['ground_pixel'] == row['ground_pixel'])]['latitude'])\n",
    "                                                       if pd.isnull(row['latitude']) else row['latitude'], \n",
    "                                                       axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longitudes in CAMS rows with scanlines and ground pixels\n",
    "match_df['longitude'] = match_df.apply(lambda row: float(sensor_coords_df[\n",
    "                                                        (sensor_coords_df['scanline'] == row['scanline']) & \n",
    "                                                        (sensor_coords_df['ground_pixel'] == row['ground_pixel'])]['longitude'])\n",
    "                                                        if pd.isnull(row['longitude']) else row['longitude'], \n",
    "                                                        axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique timestep\n",
    "sensor_times = sensor_ds_time.delta_time.isel(scanline = 0).values\n",
    "model_times = model_ds_time.valid_time.values\n",
    "unique_step = int(np.unique(nearest_neighbour(model_times, sensor_times)))\n",
    "unique_time = model_ds_time.component.isel(step = unique_step).step.values.astype('timedelta64[h]')\n",
    "\n",
    "# Get CAMS model partial columns above each level at closest TROPOMI locations (nearest neighbours)\n",
    "match_df['model_partial_column_above'] = match_df.apply(lambda row: model_ds_time.component.sel(\n",
    "                                                                    step = unique_time,\n",
    "                                                                    hybrid = row['layer'] / 1000, \n",
    "                                                                    latitude = row['latitude'], \n",
    "                                                                    longitude = row['longitude'], \n",
    "                                                                    method = 'nearest').values \n",
    "                                                                    if pd.isnull(row['sensor_column']) \n",
    "                                                                    else math.nan,\n",
    "                                                                    axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical interpolation of CAMS partial columns at pressure levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CAMS model level pressures\n",
    "match_df['pressure'] = match_df.apply(lambda row: model_ds_time.pressure.sel(\n",
    "                                                    step = unique_time,\n",
    "                                                    hybrid = row['layer'] / 1000, \n",
    "                                                    latitude = row['latitude'], \n",
    "                                                    longitude = row['longitude'], \n",
    "                                                    method = 'nearest').values \n",
    "                                                    if pd.isnull(row['pressure']) else row['pressure'],\n",
    "                                                    axis = 1)\n",
    "\n",
    "# Transform 1D-array data to float\n",
    "match_df['model_partial_column_above'] = match_df['model_partial_column_above'].apply(lambda x: float(x))\n",
    "match_df['pressure'] = match_df['pressure'].apply(lambda x: float(x))\n",
    "\n",
    "# Set multiindex again and sort for interpolation\n",
    "match_df = match_df.reset_index()\n",
    "match_df = match_df.set_index(['time', 'ground_pixel', 'scanline', 'pressure'])\n",
    "match_df = match_df.sort_values(['time', 'ground_pixel','scanline', 'pressure'], \n",
    "                                ascending = [True, True, True, False])\n",
    "\n",
    "# Interpolate partial columns onto the TM5 pressure levels.\n",
    "match_df = match_df[~match_df.index.duplicated()]\n",
    "match_df['model_partial_column_above'] = match_df['model_partial_column_above'].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary values\n",
    "match_df = match_df.reset_index()\n",
    "match_df = match_df.set_index(['time', 'ground_pixel', 'scanline', 'layer'])\n",
    "match_df = match_df.drop(np.arange(1, 137) * 1000, level = 'layer')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the averaging kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CAMS partial columns for each TM5 layer (as difference of the interpolated values)\n",
    "match_df['model_column'] =  match_df['model_partial_column_above'] - match_df['model_partial_column_above'].shift(-1)\n",
    "match_df = match_df.reset_index()\n",
    "match_df.loc[match_df['layer'] == 33, ['model_column']] = match_df['model_partial_column_above']\n",
    "match_df = match_df.set_index(['time', 'ground_pixel', 'scanline', 'layer'])\n",
    "\n",
    "# Calculate values to generate CAMS column to sum in the next step\n",
    "if 'apriori_profile' in match_df.columns:\n",
    "    match_df['model_column'] = match_df.apply(lambda row: row['apriori_profile'] +\n",
    "                                                            row['column_kernel'] * row['model_column']  -\n",
    "                                                            row['column_kernel'] * row['apriori_profile'], \n",
    "                                                            axis = 1)\n",
    "\n",
    "else:\n",
    "    match_df['model_column'] = match_df.apply(lambda row: row['model_column'] * \n",
    "                                                            row['column_kernel'], \n",
    "                                                            axis = 1)\n",
    "\n",
    "match_df = match_df[~match_df.index.duplicated()]\n",
    "match_table = match_table.append(match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total columns and difference between CAMS and TROPOMI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_table = []\n",
    "\n",
    "# Get data for timestep (in this training, only 1)\n",
    "match_ds = match_table.query('time == @time').to_xarray()\n",
    "\n",
    "# Read latitudes and longitudes from data array\n",
    "latitude = match_ds.sel(time = time).latitude.mean(dim = 'layer')\n",
    "longitude = match_ds.sel(time = time).longitude.mean(dim = 'layer')\n",
    "\n",
    "# Get sum of CAMS data of each layer to get column data\n",
    "model_final_ds_time = match_ds.sel(time = time).model_column.sum(dim = 'layer', skipna = False).astype(float)\n",
    "model_final_ds_time = model_final_ds_time.assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "# Get mean of TROPOMI data of each layer (it must be equal)\n",
    "sensor_final_ds_time = match_ds.sensor_column.sel(time = time).mean(dim = 'layer', skipna = False).astype(float)\n",
    "sensor_final_ds_time = sensor_final_ds_time.assign_coords(latitude = latitude, longitude = longitude)\n",
    "\n",
    "# Calculate difference\n",
    "merged_ds_time = xr.merge([model_final_ds_time, sensor_final_ds_time])\n",
    "merged_ds_time['difference'] = merged_ds_time.sensor_column - merged_ds_time.model_column\n",
    "merge_table.append(merged_ds_time.to_dataframe())\n",
    "\n",
    "merge_table = pd.concat(merge_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_statistics_table = merge_table.describe()\n",
    "descr_statistics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='comparison_analysis'></a>5. Comparison analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select plot dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dates = plot_period(sensor_ds, sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select plot extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox = plot_extent(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model and TROPOMI total columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose distribution (aggregated, individual or animated)\n",
    "distribution_type = 'individual'\n",
    "\n",
    "# Choose range (original, equal or manual)\n",
    "range_type = 'equal'\n",
    "vmin_manual = None\n",
    "vmax_manual = None\n",
    "\n",
    "# Define projection and colors\n",
    "projection = ccrs.PlateCarree()\n",
    "color_scale = 'coolwarm' \n",
    "\n",
    "visualize_model_vs_sensor(model, sensor, component_nom, units, merge_table, plot_dates, plot_bbox, 20, 0.80, \n",
    "                          model_type, sensor_type, range_type, distribution_type, projection,\n",
    "                          color_scale, vmin_manual, vmax_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nearest values to specific latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_search_list = (50, 60,\n",
    "                      4, 10,\n",
    "                      20, 30)\n",
    "retrieval_table_all =  retrieve_coords(merge_table.dropna(), coords_search_list, component_nom, \n",
    "                                       sensor, model, plot_dates, units)\n",
    "retrieval_table_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots by bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_seasons = False\n",
    "extent_definition = 'bbox' # bbox or country\n",
    "scatter_plot_type = 'individual' # aggregated or individual\n",
    "\n",
    "lim_min = None\n",
    "lim_max = None\n",
    "\n",
    "summary = scatter_plot(merge_table.dropna(), component_nom, units, sensor, plot_dates, 1.05, \n",
    "                       extent_definition, show_seasons, scatter_plot_type, lim_min, lim_max, plot_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_seasons = True\n",
    "extent_definition = 'bbox' # bbox or country\n",
    "scatter_plot_type = 'individual' # aggregated or individual\n",
    "\n",
    "lim_min = None\n",
    "lim_max = None\n",
    "\n",
    "summary = scatter_plot(merge_table.dropna(), component_nom, units, sensor, plot_dates, 1.05, \n",
    "                       extent_definition, show_seasons, scatter_plot_type, lim_min, lim_max, plot_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots by country (Google API required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "show_seasons = False\n",
    "extent_definition = 'country' # bbox or country\n",
    "scatter_plot_type = 'aggregated' # aggregated or individual\n",
    "plot_countries = ['Czech Republic', 'Poland', 'Germany']\n",
    "\n",
    "lim_min = None\n",
    "lim_max = None\n",
    "\n",
    "scatter_plot(merge_table.dropna(), component_nom, units, sensor, plot_dates, 1.05, \n",
    "             extent_definition, show_seasons, scatter_plot_type, lim_min, lim_max, plot_countries)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc2967d46b8688a8c6de8a18a3daae8ebe0b7dc5d18d27687b3fe01b2a6426f9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env-new': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
